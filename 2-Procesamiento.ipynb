{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, explode\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la sesión de Spark\n",
    "conf = SparkConf().set(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName('Clima Procesamiento') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Retorna ruta completa de último archivo\n",
    "        return ultimo_archivo\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "data_dir = 'Datos'\n",
    "\n",
    "ultimo_archivo = obtener_ultimo_archivo(data_dir, extension='.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|address|              alerts|   currentConditions|                days|         description|latitude|longitude|queryCost|resolvedAddress|            stations|   timezone|tzoffset|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|Sicilia|[{No Special Awar...|{88.0, Partially ...|[{62.6, Rain, Par...|Similar temperatu...| 38.1221|  13.3611|        1|Sicilia, Italia|{{0.0, 54398.0, C...|Europe/Rome|     1.0|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(ultimo_archivo)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESAMIENTO DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import array, col, concat_ws, split, size, row_number, monotonically_increasing_id,expr\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, DoubleType, LongType, FloatType\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "json_file_path = 'metadata_ingestion.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def obtener_api_key(file_path_key):\n",
    "    \n",
    "    with open(file_path_key, 'r') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "def extraer_datos_climaticos(url, params, api_key):\n",
    "    \n",
    "    locacion = params['locacion']\n",
    "    fecha = params['fecha']\n",
    "    url_final = f'{url}{locacion}/{fecha}?key={api_key}'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url_final)\n",
    "        r_json = r.json()\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            return r, r_json\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e}')\n",
    "            return None\n",
    "               \n",
    "def guardar_archivos_datos(data):\n",
    "    \n",
    "    directorio_actual = os.getcwd()\n",
    "    nombre_carpeta_archivos = 'Datos'\n",
    "    carpeta_archivos = os.path.join(directorio_actual, nombre_carpeta_archivos)\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "    nombre_archivo = f'datos_climaticos_{fecha_actual}.json'\n",
    "    ruta_archivos = os.path.join(carpeta_archivos, nombre_archivo)\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    if not os.path.exists(carpeta_archivos):\n",
    "        os.makedirs(carpeta_archivos)\n",
    "\n",
    "    try:\n",
    "        # Normalizar los datos si es necesario\n",
    "        if isinstance(data, (list, dict)):\n",
    "            with open(ruta_archivos, 'w', encoding= 'utf-8') as archivo_json:\n",
    "                json.dump(data, archivo_json, separators = (',', ':'))\n",
    "            print(f'Archivo JSON guardado en: {ruta_archivos}')\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f'Error al guardar los datos: {e}')  \n",
    "\n",
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Se crea un dataframe con el archivo Json\n",
    "        df = spark.read.json(ultimo_archivo)\n",
    "        \n",
    "        print('Se obtuvo el último archivo descargado en la carpeta \"Datos\" y se convirtió a DataFrame de Spark.')\n",
    "        # Retorna el dataframe\n",
    "        return df\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "def explotar_columnas_array(df, diccionario_resultado, sufijo_explode=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, ArrayType):\n",
    "                nombre_columna = campo.name\n",
    "                \n",
    "                alias = nombre_columna\n",
    "                diccionario_resultado[nombre_columna] = df.select(explode(col(nombre_columna)).alias(alias))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la exploción de columnas: {e}')\n",
    "            \n",
    "def desanidar_columnas_struct(df, diccionario_resultado, sufijo_desanidado=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, StructType):\n",
    "                columna_nombre = campo.name\n",
    "                \n",
    "                campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                \n",
    "                diccionario_resultado[columna_nombre] = df.select(*campos_struct)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f'Error en el desanidado de columnas: {e}')\n",
    "                    \n",
    "def aplicar_dataframe(metodo:str, diccionario_df, diccionario_dfResultado, sufijo=None):\n",
    "    \n",
    "    try:\n",
    "        if metodo == 'explotar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                explotar_columnas_array(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    explotar_columnas_array(df, diccionario_dfResultado, sufijo)\n",
    "        \n",
    "        elif metodo == 'desanidar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                desanidar_columnas_struct(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    desanidar_columnas_struct(df, diccionario_dfResultado, sufijo)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la ejecución de la función principal de exploción y desanidado de columnas: {e}')\n",
    "    \n",
    "def transformar_dfPandas(diccionario_df, nombre_dataframe=None):\n",
    "    \n",
    "    try:    \n",
    "        if nombre_dataframe:\n",
    "            nombre_df = diccionario_df[nombre_dataframe]\n",
    "            dfPandas = nombre_df.toPandas()\n",
    "            \n",
    "            print(f'DataFrame de Spark del diccionario \"{nombre_dataframe}\" convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "\n",
    "        else:\n",
    "            dfPandas = diccionario_df.toPandas()\n",
    "            \n",
    "            print('DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conversión de DF de Spark a DF de Pandas: {e}')\n",
    "                \n",
    "def unificar_df(diccionario_df=None, df1=None, df2=None):\n",
    "\n",
    "    try:    \n",
    "        if isinstance(diccionario_df, dict):\n",
    "            df_list = []  # Lista para almacenar DataFrames normalizados\n",
    "\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Obtener nombres de columnas\n",
    "                columnas_originales = df.schema.names\n",
    "\n",
    "                # Extraer el prefijo (por ejemplo, \"stations_C6242_\")\n",
    "                prefijo = key + \"_\"  # Usa la clave del diccionario como prefijo\n",
    "\n",
    "                # Verificar si las columnas realmente tienen el prefijo antes de renombrarlas\n",
    "                df_renombrado = df.select(\n",
    "                    [col(c).alias(c.replace(prefijo, \"\")) if c.startswith(prefijo) else col(c) for c in columnas_originales]\n",
    "                )\n",
    "\n",
    "                df_list.append(df_renombrado)  # Agregar el DF transformado\n",
    "\n",
    "            # Unificar todos los DataFrames en uno solo\n",
    "            if df_list:\n",
    "                df_final = df_list[0]\n",
    "                for df in df_list[1:]:\n",
    "                    df_final = df_final.unionByName(df, allowMissingColumns=True)  # Une permitiendo columnas faltantes\n",
    "                \n",
    "                print('DataFrame unificado.')\n",
    "                return df_final\n",
    "        \n",
    "        elif isinstance(df1, DataFrame) and isinstance(df2, DataFrame):\n",
    "            df_unido = df1.unionByName(df2, allowMissingColumns=True)\n",
    "            \n",
    "            print('2 DataFrames unificados.')\n",
    "            return df_unido\n",
    "        \n",
    "        else:\n",
    "            print('No se proporcionó diccionario válido ni dos DataFrames.')\n",
    "            \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ No se pudo unificar el DataFrame, revisa los datos de entrada: {e}\")\n",
    "     \n",
    "def reemplazar_nulos(diccionario_df):\n",
    "    \n",
    "    # REEMPLAZO VALORES NULOS\n",
    "    valores_reemplazo = { \n",
    "    StringType : 'Sin Dato',\n",
    "    IntegerType : 0,\n",
    "    LongType : 0,\n",
    "    DoubleType : 0.0}   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if isinstance(diccionario_df, dict):\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Reemplazar según tipo de dato\n",
    "                for columna in df.schema.fields:\n",
    "                    tipo = columna.dataType\n",
    "                    \n",
    "                    if isinstance(tipo, ArrayType) and isinstance(tipo.elementType, StringType):\n",
    "                        df = df.withColumn(\n",
    "                            columna.name,\n",
    "                            when(col(columna.name).isNull(), array(lit('Sin Dato')))\n",
    "                            .otherwise(col(columna.name))\n",
    "                        )\n",
    "                    \n",
    "                    elif isinstance(tipo, StringType):\n",
    "                        df = df.fillna({columna.name : 'Sin Dato'})\n",
    "                    \n",
    "                    elif isinstance(tipo, (IntegerType, LongType)):\n",
    "                        df = df.fillna({columna.name : 0})\n",
    "                    \n",
    "                    elif isinstance(tipo, DoubleType):\n",
    "                        df = df.fillna({columna.name : 0.0})\n",
    "            \n",
    "                \n",
    "                print(f'Valores nulos reemplazados en DataFrame (diccionario): \"{key}\".')\n",
    "                diccionario_df[key] = df\n",
    "                \n",
    "        elif hasattr(diccionario_df, 'schema'):\n",
    "            for columna in diccionario_df.schema.fields:\n",
    "                    tipo = columna.dataType\n",
    "                    \n",
    "                    if isinstance(tipo, ArrayType) and isinstance(tipo.elementType, StringType):\n",
    "                        diccionario_df = diccionario_df.withColumn(\n",
    "                            columna.name,\n",
    "                            when(col(columna.name).isNull(), array(lit('Sin Dato')))\n",
    "                            .otherwise(col(columna.name))\n",
    "                        )\n",
    "                    \n",
    "                    elif isinstance(tipo, StringType):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 'Sin Dato'})\n",
    "                    \n",
    "                    elif isinstance(tipo, (IntegerType, LongType)):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 0})\n",
    "                    \n",
    "                    elif isinstance(tipo, DoubleType):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 0.0})\n",
    "                       \n",
    "            print(f'Valores nulos reemplazados en DataFrame.')\n",
    "            return diccionario_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error en el reemplazo de valores nulos: {e}')\n",
    "              \n",
    "def eliminar_columna(diccionario_df, nombre_columna, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if nombre_dataframe:\n",
    "            if isinstance(nombre_columna, list):\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(*nombre_columna)\n",
    "                \n",
    "                print(f'Columna {nombre_columna} eliminada del DataFrame {nombre_dataframe}.')\n",
    "                return df_eliminacionColumna\n",
    "            \n",
    "            else:\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(nombre_columna)\n",
    "                \n",
    "                print(f'Columna \"{nombre_columna}\" eliminada del DataFrame \"{nombre_dataframe}\".')\n",
    "                return df_eliminacionColumna\n",
    "        \n",
    "        else:\n",
    "            df_eliminacionColumna = diccionario_df.drop(*nombre_columna)\n",
    "            \n",
    "            print(f'Eliminación correcta de columnas {nombre_columna} del DataFrame (no diccionario).')\n",
    "            return df_eliminacionColumna\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de columna \"{nombre_columna}\" del DataFrame \"{nombre_dataframe}: {e}\"')\n",
    "        \n",
    "def eliminar_corchetes_array(diccionario_df, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if isinstance(diccionario_df, DataFrame):\n",
    "            \n",
    "            columnas_array = [columna.name for columna in diccionario_df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "\n",
    "            if columnas_array:\n",
    "                for columna in columnas_array:\n",
    "                    diccionario_df = diccionario_df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\" (no era diccionario).')\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        \n",
    "        elif nombre_dataframe:\n",
    "            \n",
    "            df = diccionario_df[nombre_dataframe]\n",
    "            \n",
    "            columnas_array = [columna.name for columna in df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "            \n",
    "            if columnas_array:\n",
    "                for columna in columnas_array:\n",
    "                    df = df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                diccionario_df[nombre_dataframe] = df\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        else:\n",
    "            for key, df in diccionario_df.items():\n",
    "                columnas_array = [columna.name for columna in df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "                \n",
    "                if columnas_array:\n",
    "                    for columna in columnas_array:\n",
    "                        df = df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "        \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                diccionario_df[nombre_dataframe] = df\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        return diccionario_df\n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de corchetes de columnas Array, Dataframe: {e}')\n",
    "        \n",
    "def guardar_csv(df, ruta_directorio, nombre_archivo_base):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(ruta_directorio, exist_ok=True)\n",
    "        \n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        nombre_archivo = f'{nombre_archivo_base}_{fecha_actual}.csv'\n",
    "\n",
    "        ruta_completa = os.path.join(ruta_directorio, nombre_archivo)\n",
    "    \n",
    "        df.to_csv(ruta_completa, index = False)\n",
    "        print(f'DataFrame guardado en ruta: {ruta_completa}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en cargar el DataFrame de Pandas en formato CSV: {e}')\n",
    "     \n",
    "def obtener_ultimo_valor():\n",
    "    \n",
    "    try: \n",
    "        with open(json_file_path) as json_file:\n",
    "            data_json = json.load(json_file)\n",
    "        \n",
    "        return data_json['table_name']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en obtener el último valor: {e}')\n",
    "\n",
    "def obtener_nuevo_valor(nuevo_valor):\n",
    "    \n",
    "    fecha_actual = datetime.now()\n",
    "    valor_formatoCorrecto = fecha_actual.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return valor_formatoCorrecto      \n",
    "\n",
    "def actualizar_ultimo_valor(nuevo_valor):\n",
    "    \n",
    "    with open(json_file_path, '+r') as file_json:\n",
    "        data_json = json.load(file_json)\n",
    "        \n",
    "        data_json['table_name']['last_value'] = nuevo_valor\n",
    "        file_json.seek(0)  \n",
    "        json.dump(data_json, file_json, indent=4)\n",
    "\n",
    "def aplicar_extraccion_incremental(url, params, file_path_key):\n",
    "    \n",
    "    api_key = obtener_api_key(file_path_key)\n",
    "       \n",
    "    r, datos = extraer_datos_climaticos(url, params, api_key)\n",
    "    \n",
    "    nuevo_valor = datos['days'][0]['datetime']\n",
    "    nuevo_valor1 = obtener_nuevo_valor(nuevo_valor)\n",
    "    \n",
    "    actualizar_ultimo_valor(nuevo_valor1)\n",
    "    \n",
    "    return datos\n",
    "\n",
    "def asignar_ids_incrementales(df, columna_crear, columna_id_original=None):\n",
    "    \n",
    "    try:\n",
    "        if columna_id_original and columna_id_original in df.columns:\n",
    "            \n",
    "            window_spec = Window.orderBy(columna_id_original)\n",
    "            \n",
    "            df = df.withColumn(columna_crear, row_number().over(window_spec))\n",
    "            \n",
    "            print(f'Asignación de IDs numéricos incremental realizados con éxito para la columna \"{columna_id_original}\".')\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            df = df.withColumn(columna_crear, expr('monotonically_increasing_id() + 1'))\n",
    "            \n",
    "            print(f'Asignación de IDs numéricos incremental realizados con éxito para la columna \"{columna_crear}\".')\n",
    "            \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error en la asignación de IDs numéricos incrementales para la columna \"{columna_crear}\": {e}')\n",
    "\n",
    "\n",
    "def expandir_stations(df_spark, columna_stations):\n",
    "    \n",
    "    try:\n",
    "        df_spark = df_spark.withColumn(columna_stations, split(col(columna_stations), ', '))\n",
    "    \n",
    "        max_stations = df_spark.select(size(col(columna_stations)).alias('num_stations')).agg({'num_stations' : 'max'}).collect()[0][0]\n",
    "        \n",
    "        print(f'Número máximo de columnas \"stations\" encontradas \"{max_stations}\" para la columna \"{columna_stations}\".')\n",
    "        \n",
    "        for i in range(max_stations):\n",
    "            df_spark = df_spark.withColumn(f'{columna_stations}_{i+1}', col(columna_stations)[i])\n",
    "            \n",
    "        return df_spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la expansión de columnas \"station\" para la columna \"{columna_stations}\": {e}')\n",
    "\n",
    "\n",
    "def mapear_ids_stations(df_spark, df_stations, columna_stations_base):\n",
    "    \n",
    "    try:\n",
    "        df_spark = df_spark.alias('df_fact')\n",
    "        df_stations = df_stations.alias('df_dim')\n",
    "        \n",
    "        columnas_eliminar = []\n",
    "        \n",
    "        for i in range(1,4):\n",
    "            columna_station = f'{columna_stations_base}_{i}'\n",
    "            columnas_eliminar.append(columna_station)\n",
    "            \n",
    "            df_spark = df_spark.join(\n",
    "                        df_stations, \n",
    "                        col(f'df_fact.{columna_station}') == col('df_dim.id'),\n",
    "                        'left'\n",
    "                        ).select(\n",
    "                            df_spark['*'],\n",
    "                            df_stations['id_stations'].alias(f'stationsID_{i}')\n",
    "                        )\n",
    "        \n",
    "        df_spark = df_spark.drop(*columnas_eliminar\n",
    "                                 )\n",
    "        print(f'Mapeo de IDs para columna \"stations\" realizado con éxito para la columna \"{columna_stations_base}\".')\n",
    "        return df_spark \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en el mapeo de IDs \"station\" para la columna \"{columna_stations_base}\": {e}') \n",
    "        \n",
    "def convertir_columnas_fecha_hora(df):\n",
    "    \"\"\"\n",
    "    Convierte las columnas de un DataFrame de Pandas que contienen fechas (YYYY-MM-DD) \n",
    "    o tiempos (HH:MM:SS) a tipos datetime64[ns], manteniéndolos diferenciados.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Evitar modificar el DataFrame original\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Solo procesar columnas de texto\n",
    "            \n",
    "            # Intentar convertir a formato de fecha (YYYY-MM-DD)\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='raise')\n",
    "                df[col] = df[col].dt.date  # Convertir a objeto date\n",
    "                df[col] = df[col].astype('datetime64[ns]')  # Forzar a datetime64 para mapearlo correctamente\n",
    "                print(f'✔ Columna \"{col}\" convertida a FECHA (DATE)')\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            # Intentar convertir a formato de hora (HH:MM:SS)\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%H:%M:%S', errors='raise')\n",
    "                print(f'✔ Columna \"{col}\" convertida a HORA (TIME)')\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def mapear_tipos_datos_mysql(df):\n",
    "    \n",
    "    tipos_mysql = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        \n",
    "        if dtype.startswith('datetime64'):\n",
    "            \n",
    "            if df[col].dt.strftime('%H:%M:%S').nunique() == 1 and '00:00:00' in df[col].dt.strftime('%H:%M:%S').values:\n",
    "                tipos_mysql[col] = 'DATE'\n",
    "            \n",
    "            elif df[col].dt.strftime('%Y-%m-%d').nunique() == 1 and '1900-01-01' in df[col].dt.strftime('%Y-%m-%d').values:\n",
    "                tipos_mysql[col] = 'TIME'\n",
    "            \n",
    "            else: \n",
    "                tipos_mysql[col] = 'DATETIME'\n",
    "\n",
    "        elif dtype == 'object':\n",
    "            tipos_mysql[col] = 'VARCHAR(400)'\n",
    "        \n",
    "        elif dtype.startswith('int'):\n",
    "            tipos_mysql[col] = 'INT'\n",
    "        \n",
    "        elif dtype.startswith('float'):\n",
    "            tipos_mysql[col] = 'FLOAT'\n",
    "        \n",
    "        else:\n",
    "            tipos_mysql[col] = 'VARCHAR(400)'\n",
    "        \n",
    "    return tipos_mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo JSON guardado en: c:\\Users\\dispe\\OneDrive\\Documentos\\CLASES\\Proyecto_Procesamiento_Automatizacion\\Datos\\datos_climaticos_2025-02-06.json\n",
      "Se obtuvo el último archivo descargado en la carpeta \"Datos\" y se convirtió a DataFrame de Spark.\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"alerts\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"alerts\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days_hours\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days_preciptype\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days_stations\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"days_hours\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"currentConditions_stations\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"stations_C6242\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"stations_D2770\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"stations_LICJ\".\n",
      "Valores nulos reemplazados en DataFrame (diccionario): \"stations_LICT\".\n",
      "Columna \"days_hours\" eliminada del DataFrame \"days\".\n",
      "Eliminación correcta de columnas ['alerts', 'currentConditions', 'days', 'stations'] del DataFrame (no diccionario).\n",
      "Corchetes eliminados de columna \"days_stations\" del DataFrame \"None\" (no era diccionario).\n",
      "Corchetes eliminados de columna \"days_hours_stations\" del DataFrame \"days_hours\".\n",
      "Corchetes eliminados de columna \"currentConditions_stations\" del DataFrame \"currentConditions\".\n",
      "DataFrame unificado.\n",
      "Asignación de IDs numéricos incremental realizados con éxito para la columna \"id\".\n",
      "Asignación de IDs numéricos incremental realizados con éxito para la columna \"id_days\".\n",
      "Asignación de IDs numéricos incremental realizados con éxito para la columna \"id_daysHours\".\n",
      "Asignación de IDs numéricos incremental realizados con éxito para la columna \"id_currentConditions\".\n",
      "Asignación de IDs numéricos incremental realizados con éxito para la columna \"id_city\".\n",
      "Número máximo de columnas \"stations\" encontradas \"3\" para la columna \"currentConditions_stations\".\n",
      "Número máximo de columnas \"stations\" encontradas \"3\" para la columna \"days_stations\".\n",
      "Número máximo de columnas \"stations\" encontradas \"3\" para la columna \"days_hours_stations\".\n",
      "Mapeo de IDs para columna \"stations\" realizado con éxito para la columna \"currentConditions_stations\".\n",
      "Mapeo de IDs para columna \"stations\" realizado con éxito para la columna \"days_stations\".\n",
      "Mapeo de IDs para columna \"stations\" realizado con éxito para la columna \"days_hours_stations\".\n",
      "Eliminación correcta de columnas ['currentConditions_stations'] del DataFrame (no diccionario).\n",
      "Eliminación correcta de columnas ['days_stations'] del DataFrame (no diccionario).\n",
      "Eliminación correcta de columnas ['days_hours_stations'] del DataFrame (no diccionario).\n",
      "Valores nulos reemplazados en DataFrame.\n",
      "Valores nulos reemplazados en DataFrame.\n",
      "Valores nulos reemplazados en DataFrame.\n",
      "DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.\n",
      "DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.\n",
      "DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.\n",
      "DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.\n",
      "DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.\n",
      "✔ Columna \"days_hours_datetime\" convertida a HORA (TIME)\n",
      "✔ Columna \"days_datetime\" convertida a FECHA (DATE)\n",
      "✔ Columna \"days_sunrise\" convertida a HORA (TIME)\n",
      "✔ Columna \"days_sunset\" convertida a HORA (TIME)\n",
      "✔ Columna \"currentConditions_datetime\" convertida a HORA (TIME)\n",
      "✔ Columna \"currentConditions_sunrise\" convertida a HORA (TIME)\n",
      "✔ Columna \"currentConditions_sunset\" convertida a HORA (TIME)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/'\n",
    "\n",
    "params = {\n",
    "    'locacion' : 'Sicilia',\n",
    "    'fecha' : datetime.now().strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "# EXTRAER Y GUARDAR DATOS CRUDOS\n",
    "file_path_key = 'api_key.txt'\n",
    "data = aplicar_extraccion_incremental(url, params, file_path_key)\n",
    "\n",
    "#data = extraer_datos_climaticos(url, params, file_path_key)\n",
    "guardar_archivos_datos(data)\n",
    "\n",
    "\n",
    "# PROCESAMIENTO DE DATOS\n",
    "data_dir = 'Datos'\n",
    "df = obtener_ultimo_archivo(data_dir, extension='.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfExplodedArray_Alerts_Days_1 = {}\n",
    "dfDesanidadoStruct_Days_2 = {}\n",
    "dfExplodeArray_DaysHours_DayStation_3 = {} \n",
    "dfDesanidadoStruct_DaysHours_4 = {} \n",
    "\n",
    "dfDesanidadoStruct_Current_Station_1 = {}\n",
    "dfDesanidadoStruct_Current_Station_2 = {}\n",
    "\n",
    "dfDesanidadoStruct_Stations_2 = {}\n",
    "\n",
    "# 1 - \n",
    "aplicar_dataframe('explotar', df, dfExplodedArray_Alerts_Days_1)\n",
    "aplicar_dataframe('desanidar', df, dfDesanidadoStruct_Current_Station_1)\n",
    "aplicar_dataframe('desanidar', dfExplodedArray_Alerts_Days_1, dfDesanidadoStruct_Days_2)\n",
    "aplicar_dataframe('explotar', dfDesanidadoStruct_Days_2, dfExplodeArray_DaysHours_DayStation_3)\n",
    "aplicar_dataframe('desanidar', dfExplodeArray_DaysHours_DayStation_3, dfDesanidadoStruct_DaysHours_4)\n",
    "aplicar_dataframe('desanidar', dfDesanidadoStruct_Current_Station_1, dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "aplicar_dataframe('explotar', dfDesanidadoStruct_Current_Station_1, dfDesanidadoStruct_Current_Station_2)\n",
    "\n",
    "# 2 -\n",
    "reemplazar_nulos(dfExplodedArray_Alerts_Days_1)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Days_2)\n",
    "reemplazar_nulos(dfExplodeArray_DaysHours_DayStation_3)\n",
    "reemplazar_nulos(dfDesanidadoStruct_DaysHours_4)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Current_Station_2)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# 3 - ELIMINACIÓN DE COLUMNAS STRUCT/ARRAY\n",
    "df_Days_2_eliminacionColumna = eliminar_columna(dfDesanidadoStruct_Days_2, 'days_hours', 'days')\n",
    "df_original = eliminar_columna(df, ['alerts', 'currentConditions', 'days', 'stations'])\n",
    "\n",
    "# 4 - ELIMINACIÓN DE CORCHETES DE COLUMNAS ARRAY\n",
    "df_Days_2_Final = eliminar_corchetes_array(df_Days_2_eliminacionColumna)\n",
    "df_Days_Hours_Final = eliminar_corchetes_array(dfDesanidadoStruct_DaysHours_4, 'days_hours')\n",
    "df_currentConditions_Final = eliminar_corchetes_array(dfDesanidadoStruct_Current_Station_1, 'currentConditions')\n",
    "\n",
    "# 5 - UNIFICACIONES\n",
    "dfUnificado_stations = unificar_df(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# ASIGNACIÓN VALORES IDs INCREMENTALES A DF station\n",
    "df_asignacionID_Stations = asignar_ids_incrementales(dfUnificado_stations, 'id_stations', 'id')\n",
    "df_asignacionID_Days = asignar_ids_incrementales(df_Days_2_Final, 'id_days')\n",
    "df_asignaciondID_DaysHours = asignar_ids_incrementales(df_Days_Hours_Final['days_hours'], 'id_daysHours')\n",
    "df_asignaciondID_currentConditions = asignar_ids_incrementales(df_currentConditions_Final['currentConditions'], 'id_currentConditions')\n",
    "df_asignaciondID_Original = asignar_ids_incrementales(df_original, 'id_city')\n",
    "\n",
    "\n",
    "# 6 - EXPANSIÓN Y MAPEO DE COLUMNAS STATIONS EN LOS DISTINTOS DFs\n",
    "df_currenConditions_expansion = expandir_stations(df_asignaciondID_currentConditions, 'currentConditions_stations')\n",
    "df_Days_expansion = expandir_stations(df_asignacionID_Days, 'days_stations')\n",
    "df_daysHours_expansion = expandir_stations(df_asignaciondID_DaysHours, 'days_hours_stations')\n",
    "# 6.1 - MAPEO\n",
    "df_currentConditions_mapeo = mapear_ids_stations(df_currenConditions_expansion, df_asignacionID_Stations, 'currentConditions_stations')\n",
    "df_Days_mapeo = mapear_ids_stations(df_Days_expansion, df_asignacionID_Stations, 'days_stations')\n",
    "df_daysHours_mapeo = mapear_ids_stations(df_daysHours_expansion, df_asignacionID_Stations, 'days_hours_stations')\n",
    "\n",
    "# ELIMINACIÓN COLUMNAS YA UTILIZADAS PARA EL MAPEO DE IDs\n",
    "df_currentConditions_eliminacionColumnas = eliminar_columna(df_currentConditions_mapeo, ['currentConditions_stations'])\n",
    "df_Days_eliminacionColumna = eliminar_columna(df_Days_mapeo, ['days_stations'])\n",
    "df_daysHours_eliminacionColumna = eliminar_columna(df_daysHours_mapeo, ['days_hours_stations'])\n",
    "\n",
    "# REEMPLAZO DE NULOS LUEGO DE ASIGNACIONES DE IDs\n",
    "df_currentConditions_final = reemplazar_nulos(df_currentConditions_eliminacionColumnas)\n",
    "df_Days_final = reemplazar_nulos(df_Days_eliminacionColumna)\n",
    "df_daysHours_final = reemplazar_nulos(df_daysHours_eliminacionColumna)\n",
    "\n",
    "# TRANSFORMACIONES A PANDAS\n",
    "dfPandas_stations = transformar_dfPandas(df_asignacionID_Stations)\n",
    "dfPandas_currentConditions = transformar_dfPandas(df_currentConditions_final)\n",
    "dfPandas_Days = transformar_dfPandas(df_Days_final)\n",
    "dfPandas_DayHours = transformar_dfPandas(df_daysHours_final)\n",
    "dfPandas_Original = transformar_dfPandas(df_original)\n",
    "\n",
    "# # # GUARDAR DF DE PANDAS EN FORMATO CSV\n",
    "# guardar_csv(dfPandas_stations, 'Datos/Datos_Procesados/Stations','Stations')\n",
    "# guardar_csv(dfPandas_currentConditions, 'Datos/Datos_Procesados/CurrentConditions', 'CurrentConditions')\n",
    "# guardar_csv(dfPandas_Days, 'Datos/Datos_Procesados/Days', 'Days')\n",
    "# guardar_csv(dfPandas_DayHours, 'Datos/Datos_Procesados/DaysHours', 'DaysHours')\n",
    "# guardar_csv(dfPandas_Original, 'Datos/Datos_Procesados/Original', 'Original')\n",
    "\n",
    "# CONVERTIR COLUMNAS \"OBJECT\" DE FORMATO FECHA A DATETIME\n",
    "dfPandas_DayHours_hora = convertir_columnas_fecha_hora(dfPandas_DayHours)         \n",
    "dfPandas_Days_hora = convertir_columnas_fecha_hora(dfPandas_Days)         \n",
    "dfPandas_currentConditions_hora = convertir_columnas_fecha_hora(dfPandas_currentConditions)\n",
    "\n",
    "# CREAR DICCIONARIOS PARA CADA DF CON NOMBRES DE SUS COLUMNAS Y TIPOS DATOS CON EL QUE SE CREARÁ LA TABLA EN MYSQL\n",
    "tipos_hours = mapear_tipos_datos_mysql(dfPandas_DayHours_hora)\n",
    "tipos_days = mapear_tipos_datos_mysql(dfPandas_Days_hora)\n",
    "tipos_currentConditions = mapear_tipos_datos_mysql(dfPandas_currentConditions_hora)\n",
    "tipos_stations = mapear_tipos_datos_mysql(dfPandas_stations)\n",
    "tipos_original = mapear_tipos_datos_mysql(dfPandas_Original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE ARCHIVOS A MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando conexión a MySQL.\n",
      "Conexión exitosa\n",
      "Datos insertados correctamente en la tabla stations.\n",
      "Datos insertados correctamente en la tabla current_conditions.\n",
      "Datos insertados correctamente en la tabla days.\n",
      "Datos insertados correctamente en la tabla days_hours.\n",
      "Datos insertados correctamente en la tabla city.\n",
      "Relaciones en \"stations_relations\" insertadas correctamente.\n",
      "Datos insertados exitosamente\n",
      "Conexión cerrada con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "user = os.environ.get('MYSQL_USER')\n",
    "password = os.environ.get('MYSQL_PASSWORD')\n",
    "host = os.environ.get('MYSQL_HOST')\n",
    "database = os.environ.get('MYSQL_DATABASE')\n",
    "port = int(os.environ.get('MYSQL_PORT', 3306))\n",
    "\n",
    "\n",
    "def insertar_datos_a_tabla(cursor, tabla, columnas, datos):\n",
    "    \n",
    "    try:\n",
    "        placeholders = ', '.join(['%s'] * len(columnas))\n",
    "        insert_query = f\"INSERT INTO {tabla} ({', '.join(columnas)}) VALUES ({placeholders})\"\n",
    "\n",
    "        cursor.executemany(insert_query, datos)\n",
    "        print(f'Datos insertados correctamente en la tabla {tabla}.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error en la inserción de datos: \"{tabla}\" / {e}')\n",
    "\n",
    "\n",
    "def crear_tablas(cursor):\n",
    "    \n",
    "    try:\n",
    "        tablas = [\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS stations (\n",
    "               contribution FLOAT,\n",
    "               distance FLOAT,\n",
    "               id VARCHAR(15) UNIQUE,\n",
    "               latitude FLOAT,\n",
    "               longitude FLOAT,\n",
    "               name VARCHAR(50),\n",
    "               quality INT,\n",
    "               useCount INT,\n",
    "               id_stations INT\n",
    "            );\n",
    "            ''',\n",
    "            '''\n",
    "            CREATE TABLE IF NOT EXISTS current_conditions (\n",
    "            id_currentConditions INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            currentConditions_cloudcover FLOAT,\n",
    "            currentConditions_conditions VARCHAR(250),\n",
    "            currentConditions_datetime TIME,\n",
    "            currentConditions_datetimeEpoch INT, \n",
    "            currentConditions_dew FLOAT,\n",
    "            currentConditions_feelslike FLOAT,\n",
    "            currentConditions_humidity FLOAT,\n",
    "            currentConditions_icon VARCHAR(100),\n",
    "            currentConditions_moonphase FLOAT,\n",
    "            currentConditions_precip FLOAT,\n",
    "            currentConditions_precipprob FLOAT,\n",
    "            currentConditions_preciptype VARCHAR(100),\n",
    "            currentConditions_pressure FLOAT,\n",
    "            currentConditions_snow FLOAT, \n",
    "            currentConditions_snowdepth FLOAT,\n",
    "            currentConditions_solarenergy FLOAT,\n",
    "            currentConditions_solarradiation FLOAT,\n",
    "            currentConditions_source VARCHAR(50),\n",
    "            currentConditions_sunrise TIME,\n",
    "            currentConditions_sunriseEpoch INT, \n",
    "            currentConditions_sunset TIME,\n",
    "            currentConditions_sunsetEpoch INT,\n",
    "            currentConditions_temp FLOAT,\n",
    "            currentConditions_uvindex FLOAT, \n",
    "            currentConditions_visibility FLOAT,\n",
    "            currentConditions_winddir FLOAT,\n",
    "            currentConditions_windgust FLOAT, \n",
    "            currentConditions_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days (\n",
    "                id_days INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_cloudcover FLOAT,\n",
    "                days_conditions VARCHAR(200),\n",
    "                days_datetime DATE,\n",
    "                days_datetimeEpoch INT,\n",
    "                days_description VARCHAR(200),\n",
    "                days_dew FLOAT,\n",
    "                days_feelslike FLOAT,\n",
    "                days_feelslikemax FLOAT, \n",
    "                days_feelslikemin FLOAT, \n",
    "                days_humidity FLOAT, \n",
    "                days_icon VARCHAR(50),\n",
    "                days_moonphase FLOAT, \n",
    "                days_precip FLOAT, \n",
    "                days_precipcover FLOAT, \n",
    "                days_precipprob FLOAT,\n",
    "                days_preciptype VARCHAR(50),\n",
    "                days_pressure FLOAT, \n",
    "                days_severerisk FLOAT, \n",
    "                days_snow FLOAT, \n",
    "                days_snowdepth FLOAT, \n",
    "                days_solarenergy FLOAT,\n",
    "                days_solarradiation FLOAT,\n",
    "                days_source VARCHAR(50),\n",
    "                days_stations VARCHAR(200),\n",
    "                days_sunrise TIME,\n",
    "                days_sunriseEpoch INT,\n",
    "                days_sunset TIME,\n",
    "                days_sunsetEpoch INT,\n",
    "                days_temp FLOAT, \n",
    "                days_tempmax FLOAT,\n",
    "                days_tempmin FLOAT,\n",
    "                days_uvindex FLOAT,\n",
    "                days_visibility FLOAT,\n",
    "                days_winddir FLOAT,\n",
    "                days_windgust FLOAT,\n",
    "                days_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days_hours (\n",
    "                id_daysHours INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_hours_cloudcover FLOAT,\n",
    "                days_hours_conditions VARCHAR(200),\n",
    "                days_hours_datetime TIME,\n",
    "                days_hours_datetimeEpoch INT, \n",
    "                days_hours_dew FLOAT,\n",
    "                days_hours_feelslike FLOAT,\n",
    "                days_hours_humidity FLOAT,\n",
    "                days_hours_icon VARCHAR(200),\n",
    "                days_hours_precip FLOAT,\n",
    "                days_hours_precipprob FLOAT,\n",
    "                days_hours_preciptype VARCHAR(200),\n",
    "                days_hours_pressure FLOAT,\n",
    "                days_hours_severerisk FLOAT,\n",
    "                days_hours_snow FLOAT,\n",
    "                days_hours_snowdepth FLOAT,\n",
    "                days_hours_solarenergy FLOAT,\n",
    "                days_hours_solarradiation FLOAT,\n",
    "                days_hours_source VARCHAR(50),\n",
    "                days_hours_stations VARCHAR(200),\n",
    "                days_hours_temp FLOAT,\n",
    "                days_hours_uvindex FLOAT,\n",
    "                days_hours_visibility FLOAT,\n",
    "                days_hours_winddir FLOAT,\n",
    "                days_hours_windgust FLOAT,\n",
    "                days_hours_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS city (\n",
    "                id_city INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                address VARCHAR(50),\n",
    "                description VARCHAR(300),\n",
    "                latitude FLOAT,\n",
    "                longitude FLOAT,\n",
    "                queryCost INT, \n",
    "                resolvedAddress VARCHAR(100),\n",
    "                timezone VARCHAR(100),\n",
    "                tzoffset FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS stations_relations (\n",
    "            id_relations INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            id_stations INT NULL,\n",
    "            id_currentConditions INT NULL,\n",
    "            id_days INT NULL,\n",
    "            id_daysHours INT NULL,\n",
    "            FOREIGN KEY (id_stations) REFERENCES stations(id_stations),\n",
    "            FOREIGN KEY (id_currentConditions) REFERENCES current_conditions(id_currentConditions),\n",
    "            FOREIGN KEY (id_days) REFERENCES days(id_days),\n",
    "            FOREIGN KEY (id_daysHours) REFERENCES days_hours(id_daysHours)\n",
    "            );\n",
    "            '''\n",
    "        ]\n",
    "        \n",
    "        for query in tablas:\n",
    "            cursor.execute(query)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la creación de tablas: {e}')\n",
    "\n",
    "\n",
    "def insertar_relaciones(cursor):\n",
    "    \n",
    "    try:\n",
    "        query = ''' \n",
    "            INSERT INTO stations_relations (id_stations, id_currentConditions, id_days, id_daysHours)\n",
    "            SELECT DISTINCT s.id_stations, c.id_currentConditions, d.id_days, dh.id_daysHours\n",
    "            FROM stations s\n",
    "            LEFT JOIN current_conditions c ON FIND_IN_SET(s.id, c.currentConditions_stations)\n",
    "            LEFT JOIN days d ON FIND_IN_SET(s.id, d.days_stations)\n",
    "            LEFT JOIN days_hours dh ON FIND_IN_SET(s.id, dh.days_hours_stations);        \n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        print('Relaciones en \"stations_relations\" insertadas correctamente.')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error insertando relaciones: {e}')\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    print('Intentando conexión a MySQL.')\n",
    "    \n",
    "    try:\n",
    "        conexion = pymysql.connect(user= user, password= password,\n",
    "                                        host= host,   \n",
    "                                        database = database,\n",
    "                                        port = port)\n",
    "        print(\"Conexión exitosa\")\n",
    "    \n",
    "        cursor = conexion.cursor()\n",
    "        \n",
    "        crear_tablas(cursor)\n",
    "        \n",
    "        dataframes = {\n",
    "            'stations' : dfPandas_stations,\n",
    "            'current_conditions' : dfPandas_currentConditions,\n",
    "            'days' : dfPandas_Days,\n",
    "            'days_hours' : dfPandas_DayHours,\n",
    "            'city' : dfPandas_Original\n",
    "        }\n",
    "        \n",
    "        for tabla, df in dataframes.items():\n",
    "            \n",
    "            columnas = df.columns.tolist()\n",
    "            datos = [tuple(row) for row in df.values]\n",
    "        \n",
    "            insertar_datos_a_tabla(cursor, tabla, columnas, datos)\n",
    "        \n",
    "        insertar_relaciones(cursor)\n",
    "        conexion.commit()\n",
    "        print('Datos insertados exitosamente')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        \n",
    "    finally: \n",
    "        conexion.close()\n",
    "        print('Conexión cerrada con éxito.')\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dfPandas_DayHours_hora.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa\n",
      "Tabla \"city\" creada con éxito.\n",
      "Tabla \"stations\" creada con éxito.\n",
      "Tabla \"hours\" creada con éxito.\n",
      "Tabla \"days\" creada con éxito.\n",
      "Tabla \"current_conditions\" creada con éxito.\n",
      "Conexión cerrada con éxito.\n"
     ]
    }
   ],
   "source": [
    "# CARGA DE DATOS A MYSQL \"2\"\n",
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "user = os.environ.get('MYSQL_USER')\n",
    "password = os.environ.get('MYSQL_PASSWORD')\n",
    "host = os.environ.get('MYSQL_HOST')\n",
    "database = os.environ.get('MYSQL_DATABASE')\n",
    "port = int(os.environ.get('MYSQL_PORT', 3306))\n",
    "\n",
    "def conectar_mysql():\n",
    "    \n",
    "    try:\n",
    "        conexion = pymysql.connect(user= user, password= password,\n",
    "                                        host= host,   \n",
    "                                        database = database,\n",
    "                                        port = port)\n",
    "        print(\"Conexión exitosa\")\n",
    "        return conexion\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión a MySQL: {e}.')\n",
    "\n",
    "\n",
    "\n",
    "def crear_tablas(cursor, diccionario_columnas):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for nombre_tabla, columnas in diccionario_columnas.items():  \n",
    "            columnas_sql = []\n",
    "            foreign_keys = []\n",
    "  \n",
    "            for col, tipo in columnas.items():\n",
    "                \n",
    "                if col.startswith('id_'):\n",
    "                    columnas_sql.append(f'{col} {tipo} PRIMARY KEY AUTO_INCREMENT NOT NULL')\n",
    "                \n",
    "                elif col.startswith('stationsID_'):\n",
    "                    columnas_sql.append(f'{col} {tipo}')\n",
    "                    foreign_keys.append(f'FOREIGN KEY ({col}) REFERENCES stations(id_stations)')\n",
    "                else:\n",
    "                    columnas_sql.append(f'{col} {tipo}')\n",
    "                 \n",
    "            esquema_sql = f\"CREATE TABLE IF NOT EXISTS {nombre_tabla} (\\n \" + ',\\n '.join(columnas_sql)\n",
    "\n",
    "            if foreign_keys:\n",
    "                esquema_sql += \",\\n \" + \",\\n \".join(foreign_keys)\n",
    "            esquema_sql += \"\\n);\"\n",
    "            \n",
    "            cursor.execute(esquema_sql)\n",
    "            print(f'Tabla \"{nombre_tabla}\" creada con éxito.')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error en la creación de la tabla \"{nombre_tabla}\": {e}')\n",
    "   \n",
    "        \n",
    "def main():\n",
    "    \n",
    "    conexion = conectar_mysql()\n",
    "    \n",
    "    cursor = conexion.cursor()\n",
    "    diccionario_columnas = {\n",
    "        'city' : tipos_original,\n",
    "        'stations' : tipos_stations,\n",
    "        'hours' : tipos_hours,\n",
    "        'days' : tipos_days,\n",
    "        'current_conditions' : tipos_currentConditions\n",
    "    }\n",
    "    \n",
    "    crear_tablas(cursor, diccionario_columnas)\n",
    "    \n",
    "    cursor.close()\n",
    "    conexion.close()\n",
    "    print('Conexión cerrada con éxito.')\n",
    "    \n",
    "\n",
    "main()   \n",
    "\n",
    "    \n",
    "# 1 - REALIZAR LAS NUEVAS FUNCIONES LAS CUALES CREEN TABLAS DINAMICAMENTE EN MYSQL\n",
    "# 1.2 - ¿CÓMO APLICAR LAS RELACIONES SI YA HAY FK EN 3 DFs?\n",
    "# 1.3 - ¿CÓMO APLICAR IDs AUTOINCREMENTALES A 4 TABLAS?\n",
    "# 2 - COMPRENDER SI SE REALIZA DE MANERA CORRECTA LA INSERCION DE DATOS A TABLA \"stations_relations\"\n",
    "# 2.2 - ENTENDER COMO FUNCIONA \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_columnas = {\n",
    "        'city' : tipos_original,\n",
    "        'stations' : tipos_stations,\n",
    "        'hours' : tipos_hours,\n",
    "        'days' : tipos_days,\n",
    "        'current_conditions' : tipos_days\n",
    "    }\n",
    "\n",
    "diccionario_columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stationsID_1\n",
      "stationsID_2\n",
      "stationsID_3\n",
      "stationsID_1\n",
      "stationsID_2\n",
      "stationsID_3\n",
      "stationsID_1\n",
      "stationsID_2\n",
      "stationsID_3\n"
     ]
    }
   ],
   "source": [
    "for nombre_tabla, columnas in diccionario_columnas.items():  \n",
    "    for col, tipo in columnas.items():\n",
    "        \n",
    "        if col.startswith('stationsID_'):\n",
    "            print(col)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
