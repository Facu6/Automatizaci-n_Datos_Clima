{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, explode\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la sesión de Spark\n",
    "conf = SparkConf().set(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName('Clima Procesamiento') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Retorna ruta completa de último archivo\n",
    "        return ultimo_archivo\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "data_dir = 'Datos'\n",
    "\n",
    "ultimo_archivo = obtener_ultimo_archivo(data_dir, extension='.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|address|              alerts|   currentConditions|                days|         description|latitude|longitude|queryCost|resolvedAddress|            stations|   timezone|tzoffset|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|Sicilia|[{No Special Awar...|{100.0, Overcast,...|[{93.2, Overcast,...|Similar temperatu...| 38.1221|  13.3611|        1|Sicilia, Italia|{{0.0, 54398.0, C...|Europe/Rome|     1.0|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(ultimo_archivo)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESAMIENTO DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import array, col, concat_ws\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, DoubleType, LongType, FloatType\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "json_file_path = 'metadata_ingestion.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def obtener_api_key(file_path_key):\n",
    "    \n",
    "    with open(file_path_key, 'r') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "def extraer_datos_climaticos(url, params, api_key):\n",
    "    \n",
    "    locacion = params['locacion']\n",
    "    fecha = params['fecha']\n",
    "    url_final = f'{url}{locacion}/{fecha}?key={api_key}'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url_final)\n",
    "        r_json = r.json()\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            return r, r_json\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e}')\n",
    "            return None\n",
    "        \n",
    "        \n",
    "def guardar_archivos_datos(data):\n",
    "    \n",
    "    directorio_actual = os.getcwd()\n",
    "    nombre_carpeta_archivos = 'Datos'\n",
    "    carpeta_archivos = os.path.join(directorio_actual, nombre_carpeta_archivos)\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "    nombre_archivo = f'datos_climaticos_{fecha_actual}.json'\n",
    "    ruta_archivos = os.path.join(carpeta_archivos, nombre_archivo)\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    if not os.path.exists(carpeta_archivos):\n",
    "        os.makedirs(carpeta_archivos)\n",
    "\n",
    "    try:\n",
    "        # Normalizar los datos si es necesario\n",
    "        if isinstance(data, (list, dict)):\n",
    "            with open(ruta_archivos, 'w', encoding= 'utf-8') as archivo_json:\n",
    "                json.dump(data, archivo_json, separators = (',', ':'))\n",
    "            print(f'Archivo JSON guardado en: {ruta_archivos}')\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f'Error al guardar los datos: {e}')  \n",
    "\n",
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Se crea un dataframe con el archivo Json\n",
    "        df = spark.read.json(ultimo_archivo)\n",
    "        \n",
    "        print('Se obtuvo el último archivo descargado en la carpeta \"Datos\" y se convirtió a DataFrame de Spark.')\n",
    "        # Retorna el dataframe\n",
    "        return df\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "def explotar_columnas_array(df, diccionario_resultado, sufijo_explode=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, ArrayType):\n",
    "                nombre_columna = campo.name\n",
    "                \n",
    "                alias = f'{nombre_columna}_{sufijo_explode}' if {sufijo_explode} is not None else nombre_columna\n",
    "                \n",
    "                diccionario_resultado[nombre_columna] = df.select(explode(col(nombre_columna)).alias(alias))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la exploción de columnas: {e}')\n",
    "            \n",
    "def desanidar_columnas_struct(df, diccionario_resultado, sufijo_desanidado=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, StructType):\n",
    "                columna_nombre = campo.name\n",
    "                \n",
    "                if sufijo_desanidado:\n",
    "                    \n",
    "                   campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{sufijo_desanidado}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                else:\n",
    "                    campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                \n",
    "                diccionario_resultado[columna_nombre] = df.select(*campos_struct)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f'Error en el desanidado de columnas: {e}')\n",
    "                    \n",
    "def aplicar_dataframe(metodo:str, diccionario_df, diccionario_dfResultado, sufijo=None):\n",
    "    \n",
    "    try:\n",
    "        if metodo == 'explotar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                explotar_columnas_array(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    explotar_columnas_array(df, diccionario_dfResultado, sufijo)\n",
    "        \n",
    "        elif metodo == 'desanidar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                desanidar_columnas_struct(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    desanidar_columnas_struct(df, diccionario_dfResultado, sufijo)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la ejecución de la función principal de exploción y desanidado de columnas: {e}')\n",
    "    \n",
    "def transformar_dfPandas(diccionario_df, nombre_dataframe=None):\n",
    "    \n",
    "    try:    \n",
    "        if nombre_dataframe:\n",
    "            nombre_df = diccionario_df[nombre_dataframe]\n",
    "            dfPandas = nombre_df.toPandas()\n",
    "            \n",
    "            print(f'DataFrame de Spark del diccionario \"{nombre_dataframe}\" convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "\n",
    "        else:\n",
    "            dfPandas = diccionario_df.toPandas()\n",
    "            \n",
    "            print('DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conversión de DF de Spark a DF de Pandas: {e}')\n",
    "                \n",
    "def unificar_df(diccionario_df):\n",
    "\n",
    "    try:\n",
    "            df_list = []  # Lista para almacenar DataFrames normalizados\n",
    "\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Obtener nombres de columnas\n",
    "                columnas_originales = df.schema.names\n",
    "\n",
    "                # Extraer el prefijo (por ejemplo, \"stations_C6242_\")\n",
    "                prefijo = key + \"_\"  # Usa la clave del diccionario como prefijo\n",
    "\n",
    "                # Verificar si las columnas realmente tienen el prefijo antes de renombrarlas\n",
    "                df_renombrado = df.select(\n",
    "                    [col(c).alias(c.replace(prefijo, \"\")) if c.startswith(prefijo) else col(c) for c in columnas_originales]\n",
    "                )\n",
    "\n",
    "                df_list.append(df_renombrado)  # Agregar el DF transformado\n",
    "\n",
    "            # Unificar todos los DataFrames en uno solo\n",
    "            if df_list:\n",
    "                df_final = df_list[0]\n",
    "                for df in df_list[1:]:\n",
    "                    df_final = df_final.unionByName(df, allowMissingColumns=True)  # Une permitiendo columnas faltantes\n",
    "                \n",
    "                print('DataFrame unificado.')\n",
    "                return df_final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ No se pudo unificar el DataFrame, revisa los datos de entrada: {e}\")\n",
    "     \n",
    "def reemplazar_nulos(diccionario_df):\n",
    "    \n",
    "    # REEMPLAZO VALORES NULOS\n",
    "    valores_reemplazo = { \n",
    "    StringType : 'Sin Dato',\n",
    "    IntegerType : 0,\n",
    "    LongType : 0,\n",
    "    DoubleType : 0.0}   \n",
    "    \n",
    "    if isinstance(diccionario_df, dict):\n",
    "        \n",
    "        try:\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Reemplazar según tipo de dato\n",
    "                for columna in df.schema.fields:\n",
    "                    tipo = columna.dataType\n",
    "                    \n",
    "                    if isinstance(tipo, ArrayType) and isinstance(tipo.elementType, StringType):\n",
    "                        df = df.withColumn(\n",
    "                            columna.name,\n",
    "                            when(col(columna.name).isNull(), array(lit('Sin Dato')))\n",
    "                            .otherwise(col(columna.name))\n",
    "                        )\n",
    "                    \n",
    "                    elif isinstance(tipo, StringType):\n",
    "                        df = df.fillna({columna.name : 'Sin Dato'})\n",
    "                    \n",
    "                    elif isinstance(tipo, (IntegerType, LongType)):\n",
    "                        df = df.fillna({columna.name : 0})\n",
    "                    \n",
    "                    elif isinstance(tipo, DoubleType):\n",
    "                        df = df.fillna({columna.name : 0.0})\n",
    "            \n",
    "                \n",
    "                print(f'Valores nulos reemplazados en DataFrame \"{key}\".')\n",
    "                diccionario_df[key] = df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error en el reemplazo de valores nulos: {e}')\n",
    "              \n",
    "def eliminar_columna(diccionario_df, nombre_columna, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if nombre_dataframe:\n",
    "            if isinstance(nombre_columna, list):\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(*nombre_columna)\n",
    "                \n",
    "                print(f'Columna {nombre_columna} eliminada del DataFrame {nombre_dataframe}.')\n",
    "                return df_eliminacionColumna\n",
    "            \n",
    "            else:\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(nombre_columna)\n",
    "                \n",
    "                print(f'Columna \"{nombre_columna}\" eliminada del DataFrame \"{nombre_dataframe}\".')\n",
    "                return df_eliminacionColumna\n",
    "        \n",
    "        else:\n",
    "            df_eliminacionColumna = diccionario_df.drop(*nombre_columna)\n",
    "            \n",
    "            print(f'Eliminación correcta de columnas {nombre_columna} del DataFrame (no diccionario).')\n",
    "            return df_eliminacionColumna\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de columna \"{nombre_columna}\" del DataFrame \"{nombre_dataframe}: {e}\"')\n",
    "        \n",
    "\n",
    "def eliminar_corchetes_array(diccionario_df, columna_nueva, columna_original, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if nombre_dataframe:\n",
    "            if isinstance(columna_nueva, list) and isinstance(columna_original, list):\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                \n",
    "                for nuevo, orig in zip(columna_nueva, columna_original):\n",
    "                    df = df.withColumn(nuevo, concat_ws(', ', orig)).drop(orig)\n",
    "            \n",
    "                print(f'Corchetes eliminados de columna \"{columna_original}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df = df.withColumn(columna_nueva, concat_ws(', ', columna_original)).drop(columna_original)\n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna_original}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                return df \n",
    "        else:\n",
    "            if isinstance(columna_nueva, list) and isinstance(columna_original, list):\n",
    "                \n",
    "                for nuevo, orig in zip(columna_nueva, columna_original):\n",
    "                    diccionario_df = diccionario_df.withColumn(nuevo, concat_ws(', ', orig)).drop(orig)\n",
    "            \n",
    "                print(f'Corchetes eliminados de columna \"{columna_original}\" del DataFrame (no diccionario).')\n",
    "                return diccionario_df\n",
    "            \n",
    "            else:\n",
    "                diccionario_df = diccionario_df.withColumn(columna_nueva, concat_ws(', ', columna_original)).drop(columna_original)       \n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna_original}\" del DataFrame (no diccionario).')\n",
    "                return diccionario_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de corchetes de columnas Array: {e}')\n",
    "\n",
    "\n",
    "def guardar_csv(df, ruta_directorio, nombre_archivo_base):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(ruta_directorio, exist_ok=True)\n",
    "        \n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        nombre_archivo = f'{nombre_archivo_base}_{fecha_actual}.csv'\n",
    "\n",
    "        ruta_completa = os.path.join(ruta_directorio, nombre_archivo)\n",
    "    \n",
    "        df.to_csv(ruta_completa, index = False)\n",
    "        print(f'DataFrame guardado en ruta: {ruta_completa}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en cargar el DataFrame de Pandas en formato CSV: {e}')\n",
    "     \n",
    " \n",
    "  \n",
    "def obtener_ultimo_valor():\n",
    "    \n",
    "    try: \n",
    "        with open(json_file_path) as json_file:\n",
    "            data_json = json.load(json_file)\n",
    "        \n",
    "        return data_json['table_name']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en obtener el último valor: {e}')\n",
    "\n",
    "\n",
    "def obtener_nuevo_valor(nuevo_valor):\n",
    "    \n",
    "    fecha_actual = datetime.now()\n",
    "    valor_formatoCorrecto = fecha_actual.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return valor_formatoCorrecto      \n",
    "\n",
    "\n",
    "def actualizar_ultimo_valor(nuevo_valor):\n",
    "    \n",
    "    with open(json_file_path, '+r') as file_json:\n",
    "        data_json = json.load(file_json)\n",
    "        \n",
    "        data_json['table_name']['last_value'] = nuevo_valor\n",
    "        file_json.seek(0)  \n",
    "        json.dump(data_json, file_json, indent=4)\n",
    "\n",
    "\n",
    "def aplicar_extraccion_incremental(url, params, file_path_key):\n",
    "    \n",
    "    api_key = obtener_api_key(file_path_key)\n",
    "       \n",
    "    r, datos = extraer_datos_climaticos(url, params, api_key)\n",
    "    \n",
    "    nuevo_valor = datos['days'][0]['datetime']\n",
    "    nuevo_valor1 = obtener_nuevo_valor(nuevo_valor)\n",
    "    \n",
    "    actualizar_ultimo_valor(nuevo_valor1)\n",
    "    \n",
    "    return datos\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(r'Datos\\datos_climaticos_2025-02-01.json')\n",
    "dfExplodedArray_Alerts_Days_PRUEBA = {}\n",
    "dfDesanidadoStruct_Current_Station_PRUEBA = {}\n",
    "\n",
    "\n",
    "def explotar_columnas_array2(df, diccionario_resultado, sufijo_explode = None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, ArrayType):\n",
    "                nombre_columna = campo.name\n",
    "                \n",
    "                alias = f'{nombre_columna}_{sufijo_explode}' if {sufijo_explode} is not None else nombre_columna\n",
    "                \n",
    "                diccionario_resultado[nombre_columna] = df.select(explode(col(nombre_columna)).alias(alias))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la explosión de columnas: {e}')\n",
    "\n",
    "\n",
    "def desanidar_columnas_struct2(df, diccionario_resultado, sufijo_explode=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, StructType):\n",
    "                columna_nombre = campo.name\n",
    "                \n",
    "                if sufijo_explode:\n",
    "                    \n",
    "                   campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{sufijo_explode}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                else:\n",
    "                    campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                \n",
    "                diccionario_resultado[columna_nombre] = df.select(*campos_struct)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error desanidando las columnas: {e}')\n",
    "                        \n",
    "            \n",
    "\n",
    "explotar_columnas_array2(df, dfExplodedArray_Alerts_Days_PRUEBA, 'explodePRUEBA')\n",
    "desanidar_columnas_struct2(df, dfDesanidadoStruct_Current_Station_PRUEBA, 'desanidarPRUEBA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currentConditions': DataFrame[currentConditions_desanidarPRUEBA_cloudcover: double, currentConditions_desanidarPRUEBA_conditions: string, currentConditions_desanidarPRUEBA_datetime: string, currentConditions_desanidarPRUEBA_datetimeEpoch: bigint, currentConditions_desanidarPRUEBA_dew: double, currentConditions_desanidarPRUEBA_feelslike: double, currentConditions_desanidarPRUEBA_humidity: double, currentConditions_desanidarPRUEBA_icon: string, currentConditions_desanidarPRUEBA_moonphase: double, currentConditions_desanidarPRUEBA_precip: double, currentConditions_desanidarPRUEBA_precipprob: double, currentConditions_desanidarPRUEBA_preciptype: string, currentConditions_desanidarPRUEBA_pressure: double, currentConditions_desanidarPRUEBA_snow: double, currentConditions_desanidarPRUEBA_snowdepth: double, currentConditions_desanidarPRUEBA_solarenergy: double, currentConditions_desanidarPRUEBA_solarradiation: double, currentConditions_desanidarPRUEBA_source: string, currentConditions_desanidarPRUEBA_stations: array<string>, currentConditions_desanidarPRUEBA_sunrise: string, currentConditions_desanidarPRUEBA_sunriseEpoch: bigint, currentConditions_desanidarPRUEBA_sunset: string, currentConditions_desanidarPRUEBA_sunsetEpoch: bigint, currentConditions_desanidarPRUEBA_temp: double, currentConditions_desanidarPRUEBA_uvindex: double, currentConditions_desanidarPRUEBA_visibility: double, currentConditions_desanidarPRUEBA_winddir: double, currentConditions_desanidarPRUEBA_windgust: double, currentConditions_desanidarPRUEBA_windspeed: double],\n",
       " 'stations': DataFrame[stations_desanidarPRUEBA_C6242: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_desanidarPRUEBA_D2770: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_desanidarPRUEBA_LICJ: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_desanidarPRUEBA_LICT: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDesanidadoStruct_Current_Station_PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currentConditions': DataFrame[currentConditions_cloudcover: double, currentConditions_conditions: string, currentConditions_datetime: string, currentConditions_datetimeEpoch: bigint, currentConditions_dew: double, currentConditions_feelslike: double, currentConditions_humidity: double, currentConditions_icon: string, currentConditions_moonphase: double, currentConditions_precip: double, currentConditions_precipprob: double, currentConditions_preciptype: string, currentConditions_pressure: double, currentConditions_snow: double, currentConditions_snowdepth: double, currentConditions_solarenergy: double, currentConditions_solarradiation: double, currentConditions_source: string, currentConditions_stations: array<string>, currentConditions_sunrise: string, currentConditions_sunriseEpoch: bigint, currentConditions_sunset: string, currentConditions_sunsetEpoch: bigint, currentConditions_temp: double, currentConditions_uvindex: double, currentConditions_visibility: double, currentConditions_winddir: double, currentConditions_windgust: double, currentConditions_windspeed: double],\n",
       " 'stations': DataFrame[stations_C6242: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_D2770: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_LICJ: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>, stations_LICT: struct<contribution:double,distance:double,id:string,latitude:double,longitude:double,name:string,quality:bigint,useCount:bigint>]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDesanidadoStruct_Current_Station_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo JSON guardado en: c:\\Users\\dispe\\OneDrive\\Documentos\\CLASES\\Proyecto_Procesamiento_Automatizacion\\Datos\\datos_climaticos_2025-02-01.json\n",
      "Se obtuvo el último archivo descargado en la carpeta \"Datos\" y se convirtió a DataFrame de Spark.\n",
      "Valores nulos reemplazados en DataFrame \"alerts\".\n",
      "Valores nulos reemplazados en DataFrame \"days\".\n",
      "Valores nulos reemplazados en DataFrame \"alerts_explode1\".\n",
      "Valores nulos reemplazados en DataFrame \"days_explode1\".\n",
      "Valores nulos reemplazados en DataFrame \"days_explode1_desanidar2_hours\".\n",
      "Valores nulos reemplazados en DataFrame \"days_explode1_desanidar2_preciptype\".\n",
      "Valores nulos reemplazados en DataFrame \"days_explode1_desanidar2_stations\".\n",
      "Valores nulos reemplazados en DataFrame \"days_explode1_desanidar2_hours_desanidar2\".\n",
      "Valores nulos reemplazados en DataFrame \"currentConditions\".\n",
      "Valores nulos reemplazados en DataFrame \"stations\".\n",
      "Valores nulos reemplazados en DataFrame \"stations_desanidar1_C6242\".\n",
      "Valores nulos reemplazados en DataFrame \"stations_desanidar1_D2770\".\n",
      "Valores nulos reemplazados en DataFrame \"stations_desanidar1_LICJ\".\n",
      "Valores nulos reemplazados en DataFrame \"stations_desanidar1_LICT\".\n"
     ]
    }
   ],
   "source": [
    "url = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/'\n",
    "\n",
    "params = {\n",
    "    'locacion' : 'Sicilia',\n",
    "    'fecha' : datetime.now().strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "# EXTRAER Y GUARDAR DATOS CRUDOS\n",
    "file_path_key = 'api_key.txt'\n",
    "data = aplicar_extraccion_incremental(url, params, file_path_key)\n",
    "\n",
    "#data = extraer_datos_climaticos(url, params, file_path_key)\n",
    "guardar_archivos_datos(data)\n",
    "\n",
    "\n",
    "# PROCESAMIENTO DE DATOS\n",
    "data_dir = 'Datos'\n",
    "df = obtener_ultimo_archivo(data_dir, extension='.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfExplodedArray_Alerts_Days_1 = {}\n",
    "dfDesanidadoStruct_Days_2 = {}\n",
    "dfExplodeArray_DaysHours_DayStation_3 = {} \n",
    "dfDesanidadoStruct_DaysHours_4 = {} \n",
    "\n",
    "dfDesanidadoStruct_Current_Station_1 = {}\n",
    "\n",
    "dfDesanidadoStruct_Stations_2 = {}\n",
    "\n",
    "\n",
    "columnas_array_1 = ['alerts', 'days']\n",
    "columnas_struct_1 = ['currentConditions', 'stations']\n",
    "\n",
    "columnas_struct_2 = ['days']\n",
    "columnas_array_2 = ['days_hours', 'days_stations']\n",
    "\n",
    "columnas_struct_3 = ['days_hours']\n",
    "\n",
    "columnas_struct_4 = {'stations_C6242', 'stations_D2770', 'stations_LICJ', 'stations_LICT'}\n",
    "\n",
    "\n",
    "aplicar_dataframe('explotar', df, dfExplodedArray_Alerts_Days_1, 'explode1')\n",
    "aplicar_dataframe('desanidar', df, dfDesanidadoStruct_Current_Station_1, 'desanidar1')\n",
    "aplicar_dataframe('desanidar', dfExplodedArray_Alerts_Days_1, dfDesanidadoStruct_Days_2, 'desanidar2')\n",
    "aplicar_dataframe('explotar', dfDesanidadoStruct_Days_2, dfExplodeArray_DaysHours_DayStation_3, 'desanidar2')\n",
    "aplicar_dataframe('desanidar', dfExplodeArray_DaysHours_DayStation_3, dfDesanidadoStruct_DaysHours_4, 'desanidar2')\n",
    "aplicar_dataframe('desanidar', dfDesanidadoStruct_Current_Station_1, dfDesanidadoStruct_Stations_2, 'desanidar2')\n",
    "\n",
    "\n",
    "reemplazar_nulos(dfExplodedArray_Alerts_Days_1)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Days_2)\n",
    "reemplazar_nulos(dfExplodeArray_DaysHours_DayStation_3)\n",
    "reemplazar_nulos(dfDesanidadoStruct_DaysHours_4)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Current_Station_1)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# # ELIMINACIÓN DE COLUMNAS STRUCT/ARRAY\n",
    "# df_Days_2_eliminacionColumna = eliminar_columna(dfDesanidadoStruct_Days_2, 'days_hours', 'days')\n",
    "# df_original = eliminar_columna(df, ['alerts', 'currentConditions', 'days', 'stations'])\n",
    "\n",
    "# # ELIMINACIÓN DE CORCHETES DE COLUMNAS ARRAY\n",
    "# df_Days_2_Final = eliminar_corchetes_array(df_Days_2_eliminacionColumna, ['days_preciptype1', 'days_stations1'], ['days_preciptype', 'days_stations'])\n",
    "# df_Days_Hours_Final = eliminar_corchetes_array(dfDesanidadoStruct_DaysHours_4, ['days_hours_preciptype1', 'days_hours_stations1'], ['days_hours_preciptype', 'days_hours_stations'], 'days_hours')\n",
    "# df_currentConditions_Final = eliminar_corchetes_array(dfDesanidadoStruct_Current_Station_1, 'currentConditions_stations1', 'currentConditions_stations', 'currentConditions')\n",
    "# # UNIFICACIONES\n",
    "# dfUnificado_stations = unificar_df(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# # TRANSFORMACIONES A PANDAS\n",
    "# dfPandas_stations = transformar_dfPandas(dfUnificado_stations)\n",
    "# dfPandas_currentConditions = transformar_dfPandas(df_currentConditions_Final)\n",
    "# dfPandas_Days = transformar_dfPandas(df_Days_2_Final)\n",
    "# dfPandas_DayHours = transformar_dfPandas(df_Days_Hours_Final)\n",
    "# dfPandas_Original = transformar_dfPandas(df_original)\n",
    "\n",
    "# # GUARDAR DF DE PANDAS EN FORMATO CSV\n",
    "# guardar_csv(dfPandas_stations, 'Datos/Datos_Procesados/Stations','Stations')\n",
    "# guardar_csv(dfPandas_currentConditions, 'Datos/Datos_Procesados/CurrentConditions', 'CurrentConditions')\n",
    "# guardar_csv(dfPandas_Days, 'Datos/Datos_Procesados/Days', 'Days')\n",
    "# guardar_csv(dfPandas_DayHours, 'Datos/Datos_Procesados/DaysHours', 'DaysHours')\n",
    "# guardar_csv(dfPandas_Original, 'Datos/Datos_Procesados/Original', 'Original')\n",
    "\n",
    "\n",
    "# 1 - APLICAR CARGA INCREMENTAL A LOS DATOS (MYSQL)\n",
    "\n",
    "# 2 - A LA HORA DE HACER EL DESANIDADO O EXPLOSION, CÓMO HACER PARA NO DETALLAR COLUMNAS EXPLICITAMENTE\n",
    "#      YA QUE EN CASO QUE SEAN OTRAS LAS COLUMNAS DARÁ ERROR\n",
    "\n",
    "# 3 - APLICAR SEGURIDAD EN DATOS DE CONEXIÓN A MYSQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE ARCHIVOS A MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando conexión a MySQL.\n",
      "Conexión exitosa\n",
      "Datos insertados correctamente en la tabla stations.\n",
      "Datos insertados correctamente en la tabla current_conditions.\n",
      "Datos insertados correctamente en la tabla days.\n",
      "Datos insertados correctamente en la tabla days_hours.\n",
      "Datos insertados correctamente en la tabla city.\n",
      "Datos insertados exitosamente\n",
      "Conexión cerrada con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "def insertar_datos_a_tabla(cursor, tabla, columnas, datos):\n",
    "    \n",
    "    try:\n",
    "        placeholders = ', '.join(['%s'] * len(columnas))\n",
    "        insert_query = f\"INSERT INTO {tabla} ({', '.join(columnas)}) VALUES ({placeholders})\"\n",
    "\n",
    "        cursor.executemany(insert_query, datos)\n",
    "        print(f'Datos insertados correctamente en la tabla {tabla}.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error en la inserción de datos: \"{tabla}\" / {e}')\n",
    "\n",
    "\n",
    "def crear_tablas(cursor):\n",
    "    \n",
    "    try:\n",
    "        tablas = [\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS stations (\n",
    "               id_stations INT AUTO_INCREMENT PRIMARY KEY,\n",
    "               contribution FLOAT,\n",
    "               distance FLOAT,\n",
    "               id VARCHAR(15),\n",
    "               latitude FLOAT,\n",
    "               longitude FLOAT,\n",
    "               name VARCHAR(50),\n",
    "               quality INT,\n",
    "               useCount INT\n",
    "            );\n",
    "            ''',\n",
    "            '''\n",
    "            CREATE TABLE IF NOT EXISTS current_conditions (\n",
    "            id_currentConditions INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            currentConditions_cloudcover FLOAT,\n",
    "            currentConditions_conditions VARCHAR(250),\n",
    "            currentConditions_datetime TIME,\n",
    "            currentConditions_datetimeEpoch INT, \n",
    "            currentConditions_dew FLOAT,\n",
    "            currentConditions_feelslike FLOAT,\n",
    "            currentConditions_humidity FLOAT,\n",
    "            currentConditions_icon VARCHAR(100),\n",
    "            currentConditions_moonphase FLOAT,\n",
    "            currentConditions_precip FLOAT,\n",
    "            currentConditions_precipprob FLOAT,\n",
    "            currentConditions_preciptype VARCHAR(100),\n",
    "            currentConditions_pressure FLOAT,\n",
    "            currentConditions_snow FLOAT, \n",
    "            currentConditions_snowdepth FLOAT,\n",
    "            currentConditions_solarenergy FLOAT,\n",
    "            currentConditions_solarradiation FLOAT,\n",
    "            currentConditions_source VARCHAR(50),\n",
    "            currentConditions_sunrise TIME,\n",
    "            currentConditions_sunriseEpoch INT, \n",
    "            currentConditions_sunset TIME,\n",
    "            currentConditions_sunsetEpoch INT,\n",
    "            currentConditions_temp FLOAT,\n",
    "            currentConditions_uvindex FLOAT, \n",
    "            currentConditions_visibility FLOAT,\n",
    "            currentConditions_winddir FLOAT,\n",
    "            currentConditions_windgust FLOAT, \n",
    "            currentConditions_windspeed FLOAT,\n",
    "            currentConditions_stations1 VARCHAR(100)\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days (\n",
    "                id_days INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_cloudcover FLOAT,\n",
    "                days_conditions VARCHAR(200),\n",
    "                days_datetime DATE,\n",
    "                days_datetimeEpoch INT,\n",
    "                days_description VARCHAR(200),\n",
    "                days_dew FLOAT,\n",
    "                days_feelslike FLOAT,\n",
    "                days_feelslikemax FLOAT, \n",
    "                days_feelslikemin FLOAT, \n",
    "                days_humidity FLOAT, \n",
    "                days_icon VARCHAR(50),\n",
    "                days_moonphase FLOAT, \n",
    "                days_precip FLOAT, \n",
    "                days_precipcover FLOAT, \n",
    "                days_precipprob FLOAT,\n",
    "                days_pressure FLOAT, \n",
    "                days_severerisk FLOAT, \n",
    "                days_snow FLOAT, \n",
    "                days_snowdepth FLOAT, \n",
    "                days_solarenergy FLOAT,\n",
    "                days_solarradiation FLOAT,\n",
    "                days_source VARCHAR(50),\n",
    "                days_sunrise TIME,\n",
    "                days_sunriseEpoch INT,\n",
    "                days_sunset TIME,\n",
    "                days_sunsetEpoch INT,\n",
    "                days_temp FLOAT, \n",
    "                days_tempmax FLOAT,\n",
    "                days_tempmin FLOAT,\n",
    "                days_uvindex FLOAT,\n",
    "                days_visibility FLOAT,\n",
    "                days_winddir FLOAT,\n",
    "                days_windgust FLOAT,\n",
    "                days_windspeed FLOAT,\n",
    "                days_preciptype1 VARCHAR(50),\n",
    "                days_stations1 VARCHAR(200)\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days_hours (\n",
    "                id_days INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_hours_cloudcover FLOAT,\n",
    "                days_hours_conditions VARCHAR(200),\n",
    "                days_hours_datetime TIME,\n",
    "                days_hours_datetimeEpoch INT, \n",
    "                days_hours_dew FLOAT,\n",
    "                days_hours_feelslike FLOAT,\n",
    "                days_hours_humidity FLOAT,\n",
    "                days_hours_icon VARCHAR(200),\n",
    "                days_hours_precip FLOAT,\n",
    "                days_hours_precipprob FLOAT,\n",
    "                days_hours_pressure FLOAT,\n",
    "                days_hours_severerisk FLOAT,\n",
    "                days_hours_snow FLOAT,\n",
    "                days_hours_snowdepth FLOAT,\n",
    "                days_hours_solarenergy FLOAT,\n",
    "                days_hours_solarradiation FLOAT,\n",
    "                days_hours_source VARCHAR(50),\n",
    "                days_hours_temp FLOAT,\n",
    "                days_hours_uvindex FLOAT,\n",
    "                days_hours_visibility FLOAT,\n",
    "                days_hours_winddir FLOAT,\n",
    "                days_hours_windgust FLOAT,\n",
    "                days_hours_windspeed FLOAT,\n",
    "                days_hours_preciptype1 VARCHAR(200),\n",
    "                days_hours_stations1 VARCHAR(200)\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS city (\n",
    "                id_city INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                address VARCHAR(50),\n",
    "                description VARCHAR(300),\n",
    "                latitude FLOAT,\n",
    "                longitude FLOAT,\n",
    "                queryCost INT, \n",
    "                resolvedAddress VARCHAR(100),\n",
    "                timezone VARCHAR(100),\n",
    "                tzoffset FLOAT\n",
    "            );\n",
    "            '''\n",
    "        ]\n",
    "        \n",
    "        for query in tablas:\n",
    "            cursor.execute(query)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la creación de tablas: {e}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print('Intentando conexión a MySQL.')\n",
    "    \n",
    "    try:\n",
    "        conexion = pymysql.connect(user='root', password='Lisandrotorre478-',\n",
    "                                        host= 'localhost',   \n",
    "                                        database = 'proyecto_clima',\n",
    "                                        port = 3306)\n",
    "        print(\"Conexión exitosa\")\n",
    "    \n",
    "        cursor = conexion.cursor()\n",
    "        \n",
    "        crear_tablas(cursor)\n",
    "        \n",
    "        dataframes = {\n",
    "            'stations' : dfPandas_stations,\n",
    "            'current_conditions' : dfPandas_currentConditions,\n",
    "            'days' : dfPandas_Days,\n",
    "            'days_hours' : dfPandas_DayHours,\n",
    "            'city' : dfPandas_Original\n",
    "        }\n",
    "        \n",
    "        for tabla, df in dataframes.items():\n",
    "            \n",
    "            columnas = df.columns.tolist()\n",
    "            datos = [tuple(row) for row in df.values]\n",
    "        \n",
    "            insertar_datos_a_tabla(cursor, tabla, columnas, datos)\n",
    "        \n",
    "        conexion.commit()\n",
    "        print('Datos insertados exitosamente')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        \n",
    "    finally: \n",
    "        conexion.close()\n",
    "        print('Conexión cerrada con éxito.')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
