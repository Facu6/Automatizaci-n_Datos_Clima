{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, explode\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la sesión de Spark\n",
    "conf = SparkConf().set(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName('Clima Procesamiento') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Retorna ruta completa de último archivo\n",
    "        return ultimo_archivo\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "data_dir = 'Datos'\n",
    "\n",
    "ultimo_archivo = obtener_ultimo_archivo(data_dir, extension='.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|address|              alerts|   currentConditions|                days|         description|latitude|longitude|queryCost|resolvedAddress|            stations|   timezone|tzoffset|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "|Sicilia|[{No Special Awar...|{88.0, Partially ...|[{62.6, Rain, Par...|Similar temperatu...| 38.1221|  13.3611|        1|Sicilia, Italia|{{0.0, 54398.0, C...|Europe/Rome|     1.0|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------+---------+---------+---------------+--------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(ultimo_archivo)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESAMIENTO DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import array, col, concat_ws, split, size, row_number\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, DoubleType, LongType, FloatType\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "json_file_path = 'metadata_ingestion.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def obtener_api_key(file_path_key):\n",
    "    \n",
    "    with open(file_path_key, 'r') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "def extraer_datos_climaticos(url, params, api_key):\n",
    "    \n",
    "    locacion = params['locacion']\n",
    "    fecha = params['fecha']\n",
    "    url_final = f'{url}{locacion}/{fecha}?key={api_key}'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url_final)\n",
    "        r_json = r.json()\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            return r, r_json\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e}')\n",
    "            return None\n",
    "               \n",
    "def guardar_archivos_datos(data):\n",
    "    \n",
    "    directorio_actual = os.getcwd()\n",
    "    nombre_carpeta_archivos = 'Datos'\n",
    "    carpeta_archivos = os.path.join(directorio_actual, nombre_carpeta_archivos)\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "    nombre_archivo = f'datos_climaticos_{fecha_actual}.json'\n",
    "    ruta_archivos = os.path.join(carpeta_archivos, nombre_archivo)\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    if not os.path.exists(carpeta_archivos):\n",
    "        os.makedirs(carpeta_archivos)\n",
    "\n",
    "    try:\n",
    "        # Normalizar los datos si es necesario\n",
    "        if isinstance(data, (list, dict)):\n",
    "            with open(ruta_archivos, 'w', encoding= 'utf-8') as archivo_json:\n",
    "                json.dump(data, archivo_json, separators = (',', ':'))\n",
    "            print(f'Archivo JSON guardado en: {ruta_archivos}')\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f'Error al guardar los datos: {e}')  \n",
    "\n",
    "def obtener_ultimo_archivo(directorio, extension = '*.json'):\n",
    "    \n",
    "    '''\n",
    "    Obtiene el archivo más reciente de un directorio con la extensión especificada.\n",
    "\n",
    "    Args:\n",
    "        directorio (str): Ruta del directorio donde buscar los archivos.\n",
    "        extension (str): Extensión de los archivos a buscar (por defecto '*.json').\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta completa del archivo más reciente con la extensión especificada.\n",
    "        '''\n",
    "    \n",
    "    try:\n",
    "        # Obtener la lista de rutas completas de los archivos que coinciden con la extensión\n",
    "        archivos = [os.path.join(directorio, archivo) for archivo in os.listdir(directorio) if archivo.endswith(extension)]\n",
    "        \n",
    "        # Verificar si no hay archivos en la lista\n",
    "        if not archivos:\n",
    "            raise FileNotFoundError('No se encontraron archivos en el directorio especificado.')\n",
    "        \n",
    "        # Obtener el archivo más reciente basado en la fecha de modificación\n",
    "        ultimo_archivo = max(archivos, key = os.path.getmtime)\n",
    "        \n",
    "        # Se crea un dataframe con el archivo Json\n",
    "        df = spark.read.json(ultimo_archivo)\n",
    "        \n",
    "        print('Se obtuvo el último archivo descargado en la carpeta \"Datos\" y se convirtió a DataFrame de Spark.')\n",
    "        # Retorna el dataframe\n",
    "        return df\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'Error inesperado: {e}')\n",
    "        raise\n",
    "\n",
    "def explotar_columnas_array(df, diccionario_resultado, sufijo_explode=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, ArrayType):\n",
    "                nombre_columna = campo.name\n",
    "                \n",
    "                alias = nombre_columna\n",
    "                diccionario_resultado[nombre_columna] = df.select(explode(col(nombre_columna)).alias(alias))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la exploción de columnas: {e}')\n",
    "            \n",
    "def desanidar_columnas_struct(df, diccionario_resultado, sufijo_desanidado=None):\n",
    "    \n",
    "    try:\n",
    "        for campo in df.schema:\n",
    "            if isinstance(campo.dataType, StructType):\n",
    "                columna_nombre = campo.name\n",
    "                \n",
    "                campos_struct = [\n",
    "                        col(f'{columna_nombre}.{subfield.name}').alias(f'{columna_nombre}_{subfield.name}')\n",
    "                        for subfield in campo.dataType.fields\n",
    "                    ]\n",
    "                \n",
    "                diccionario_resultado[columna_nombre] = df.select(*campos_struct)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f'Error en el desanidado de columnas: {e}')\n",
    "                    \n",
    "def aplicar_dataframe(metodo:str, diccionario_df, diccionario_dfResultado, sufijo=None):\n",
    "    \n",
    "    try:\n",
    "        if metodo == 'explotar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                explotar_columnas_array(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    explotar_columnas_array(df, diccionario_dfResultado, sufijo)\n",
    "        \n",
    "        elif metodo == 'desanidar':\n",
    "            if isinstance(diccionario_df, DataFrame):\n",
    "                desanidar_columnas_struct(diccionario_df, diccionario_dfResultado, sufijo)\n",
    "            \n",
    "            elif isinstance(diccionario_df, dict):\n",
    "                for key, df in diccionario_df.items():\n",
    "                    desanidar_columnas_struct(df, diccionario_dfResultado, sufijo)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la ejecución de la función principal de exploción y desanidado de columnas: {e}')\n",
    "    \n",
    "def transformar_dfPandas(diccionario_df, nombre_dataframe=None):\n",
    "    \n",
    "    try:    \n",
    "        if nombre_dataframe:\n",
    "            nombre_df = diccionario_df[nombre_dataframe]\n",
    "            dfPandas = nombre_df.toPandas()\n",
    "            \n",
    "            print(f'DataFrame de Spark del diccionario \"{nombre_dataframe}\" convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "\n",
    "        else:\n",
    "            dfPandas = diccionario_df.toPandas()\n",
    "            \n",
    "            print('DataFrame de Spark (no diccionario) convertido a DataFrame de Pandas.')\n",
    "            return dfPandas\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conversión de DF de Spark a DF de Pandas: {e}')\n",
    "                \n",
    "def unificar_df(diccionario_df=None, df1=None, df2=None):\n",
    "\n",
    "    try:    \n",
    "        if isinstance(diccionario_df, dict):\n",
    "            df_list = []  # Lista para almacenar DataFrames normalizados\n",
    "\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Obtener nombres de columnas\n",
    "                columnas_originales = df.schema.names\n",
    "\n",
    "                # Extraer el prefijo (por ejemplo, \"stations_C6242_\")\n",
    "                prefijo = key + \"_\"  # Usa la clave del diccionario como prefijo\n",
    "\n",
    "                # Verificar si las columnas realmente tienen el prefijo antes de renombrarlas\n",
    "                df_renombrado = df.select(\n",
    "                    [col(c).alias(c.replace(prefijo, \"\")) if c.startswith(prefijo) else col(c) for c in columnas_originales]\n",
    "                )\n",
    "\n",
    "                df_list.append(df_renombrado)  # Agregar el DF transformado\n",
    "\n",
    "            # Unificar todos los DataFrames en uno solo\n",
    "            if df_list:\n",
    "                df_final = df_list[0]\n",
    "                for df in df_list[1:]:\n",
    "                    df_final = df_final.unionByName(df, allowMissingColumns=True)  # Une permitiendo columnas faltantes\n",
    "                \n",
    "                print('DataFrame unificado.')\n",
    "                return df_final\n",
    "        \n",
    "        elif isinstance(df1, DataFrame) and isinstance(df2, DataFrame):\n",
    "            df_unido = df1.unionByName(df2, allowMissingColumns=True)\n",
    "            \n",
    "            print('2 DataFrames unificados.')\n",
    "            return df_unido\n",
    "        \n",
    "        else:\n",
    "            print('No se proporcionó diccionario válido ni dos DataFrames.')\n",
    "            \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ No se pudo unificar el DataFrame, revisa los datos de entrada: {e}\")\n",
    "     \n",
    "def reemplazar_nulos(diccionario_df):\n",
    "    \n",
    "    # REEMPLAZO VALORES NULOS\n",
    "    valores_reemplazo = { \n",
    "    StringType : 'Sin Dato',\n",
    "    IntegerType : 0,\n",
    "    LongType : 0,\n",
    "    DoubleType : 0.0}   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if isinstance(diccionario_df, dict):\n",
    "            for key, df in diccionario_df.items():\n",
    "                # Reemplazar según tipo de dato\n",
    "                for columna in df.schema.fields:\n",
    "                    tipo = columna.dataType\n",
    "                    \n",
    "                    if isinstance(tipo, ArrayType) and isinstance(tipo.elementType, StringType):\n",
    "                        df = df.withColumn(\n",
    "                            columna.name,\n",
    "                            when(col(columna.name).isNull(), array(lit('Sin Dato')))\n",
    "                            .otherwise(col(columna.name))\n",
    "                        )\n",
    "                    \n",
    "                    elif isinstance(tipo, StringType):\n",
    "                        df = df.fillna({columna.name : 'Sin Dato'})\n",
    "                    \n",
    "                    elif isinstance(tipo, (IntegerType, LongType)):\n",
    "                        df = df.fillna({columna.name : 0})\n",
    "                    \n",
    "                    elif isinstance(tipo, DoubleType):\n",
    "                        df = df.fillna({columna.name : 0.0})\n",
    "            \n",
    "                \n",
    "                print(f'Valores nulos reemplazados en DataFrame (diccionario): \"{key}\".')\n",
    "                diccionario_df[key] = df\n",
    "                \n",
    "        elif hasattr(diccionario_df, 'schema'):\n",
    "            for columna in diccionario_df.schema.fields:\n",
    "                    tipo = columna.dataType\n",
    "                    \n",
    "                    if isinstance(tipo, ArrayType) and isinstance(tipo.elementType, StringType):\n",
    "                        diccionario_df = diccionario_df.withColumn(\n",
    "                            columna.name,\n",
    "                            when(col(columna.name).isNull(), array(lit('Sin Dato')))\n",
    "                            .otherwise(col(columna.name))\n",
    "                        )\n",
    "                    \n",
    "                    elif isinstance(tipo, StringType):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 'Sin Dato'})\n",
    "                    \n",
    "                    elif isinstance(tipo, (IntegerType, LongType)):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 0})\n",
    "                    \n",
    "                    elif isinstance(tipo, DoubleType):\n",
    "                        diccionario_df = diccionario_df.fillna({columna.name : 0.0})\n",
    "                       \n",
    "            print(f'Valores nulos reemplazados en DataFrame.')\n",
    "            return diccionario_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error en el reemplazo de valores nulos: {e}')\n",
    "              \n",
    "def eliminar_columna(diccionario_df, nombre_columna, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if nombre_dataframe:\n",
    "            if isinstance(nombre_columna, list):\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(*nombre_columna)\n",
    "                \n",
    "                print(f'Columna {nombre_columna} eliminada del DataFrame {nombre_dataframe}.')\n",
    "                return df_eliminacionColumna\n",
    "            \n",
    "            else:\n",
    "                df = diccionario_df[nombre_dataframe]\n",
    "                df_eliminacionColumna = df.drop(nombre_columna)\n",
    "                \n",
    "                print(f'Columna \"{nombre_columna}\" eliminada del DataFrame \"{nombre_dataframe}\".')\n",
    "                return df_eliminacionColumna\n",
    "        \n",
    "        else:\n",
    "            df_eliminacionColumna = diccionario_df.drop(*nombre_columna)\n",
    "            \n",
    "            print(f'Eliminación correcta de columnas {nombre_columna} del DataFrame (no diccionario).')\n",
    "            return df_eliminacionColumna\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de columna \"{nombre_columna}\" del DataFrame \"{nombre_dataframe}: {e}\"')\n",
    "        \n",
    "def eliminar_corchetes_array(diccionario_df, nombre_dataframe=None):\n",
    "    \n",
    "    try:\n",
    "        if isinstance(diccionario_df, DataFrame):\n",
    "            \n",
    "            columnas_array = [columna.name for columna in diccionario_df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "\n",
    "            if columnas_array:\n",
    "                for columna in columnas_array:\n",
    "                    diccionario_df = diccionario_df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\" (no era diccionario).')\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        \n",
    "        elif nombre_dataframe:\n",
    "            \n",
    "            df = diccionario_df[nombre_dataframe]\n",
    "            \n",
    "            columnas_array = [columna.name for columna in df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "            \n",
    "            if columnas_array:\n",
    "                for columna in columnas_array:\n",
    "                    df = df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "                \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                diccionario_df[nombre_dataframe] = df\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        else:\n",
    "            for key, df in diccionario_df.items():\n",
    "                columnas_array = [columna.name for columna in df.schema.fields if isinstance(columna.dataType, ArrayType)]\n",
    "                \n",
    "                if columnas_array:\n",
    "                    for columna in columnas_array:\n",
    "                        df = df.withColumn(columna, concat_ws(', ', col(columna)))\n",
    "        \n",
    "                print(f'Corchetes eliminados de columna \"{columna}\" del DataFrame \"{nombre_dataframe}\".')\n",
    "                diccionario_df[nombre_dataframe] = df\n",
    "            else:\n",
    "                print(f'No se encontraron columnas tipo Array en el DataFrame \"{nombre_dataframe}\".')\n",
    "        \n",
    "        return diccionario_df\n",
    "    except Exception as e:\n",
    "        print(f'Error en la eliminación de corchetes de columnas Array, Dataframe: {e}')\n",
    "        \n",
    "def guardar_csv(df, ruta_directorio, nombre_archivo_base):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(ruta_directorio, exist_ok=True)\n",
    "        \n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        nombre_archivo = f'{nombre_archivo_base}_{fecha_actual}.csv'\n",
    "\n",
    "        ruta_completa = os.path.join(ruta_directorio, nombre_archivo)\n",
    "    \n",
    "        df.to_csv(ruta_completa, index = False)\n",
    "        print(f'DataFrame guardado en ruta: {ruta_completa}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en cargar el DataFrame de Pandas en formato CSV: {e}')\n",
    "     \n",
    "def obtener_ultimo_valor():\n",
    "    \n",
    "    try: \n",
    "        with open(json_file_path) as json_file:\n",
    "            data_json = json.load(json_file)\n",
    "        \n",
    "        return data_json['table_name']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en obtener el último valor: {e}')\n",
    "\n",
    "def obtener_nuevo_valor(nuevo_valor):\n",
    "    \n",
    "    fecha_actual = datetime.now()\n",
    "    valor_formatoCorrecto = fecha_actual.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return valor_formatoCorrecto      \n",
    "\n",
    "def actualizar_ultimo_valor(nuevo_valor):\n",
    "    \n",
    "    with open(json_file_path, '+r') as file_json:\n",
    "        data_json = json.load(file_json)\n",
    "        \n",
    "        data_json['table_name']['last_value'] = nuevo_valor\n",
    "        file_json.seek(0)  \n",
    "        json.dump(data_json, file_json, indent=4)\n",
    "\n",
    "def aplicar_extraccion_incremental(url, params, file_path_key):\n",
    "    \n",
    "    api_key = obtener_api_key(file_path_key)\n",
    "       \n",
    "    r, datos = extraer_datos_climaticos(url, params, api_key)\n",
    "    \n",
    "    nuevo_valor = datos['days'][0]['datetime']\n",
    "    nuevo_valor1 = obtener_nuevo_valor(nuevo_valor)\n",
    "    \n",
    "    actualizar_ultimo_valor(nuevo_valor1)\n",
    "    \n",
    "    return datos\n",
    "\n",
    "def asignar_ids_incrementales_stations(df_station, columna_id_original, columna_crear):\n",
    "    \n",
    "    try:\n",
    "        window_spec = Window.orderBy(columna_id_original)\n",
    "        \n",
    "        df_station = df_station.withColumn(columna_crear, row_number().over(window_spec))\n",
    "        \n",
    "        print(f'Asignación de IDs numéricos incremental realizados con éxito para la columna \"{columna_id_original}\".')\n",
    "        return df_station\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la asignación de IDs numéricos incrementales para la columna \"{columna_id_original}\": {e}')\n",
    "\n",
    "\n",
    "def expandir_stations(df_spark, columna_stations):\n",
    "    \n",
    "    try:\n",
    "        df_spark = df_spark.withColumn(columna_stations, split(col(columna_stations), ', '))\n",
    "    \n",
    "        max_stations = df_spark.select(size(col(columna_stations)).alias('num_stations')).agg({'num_stations' : 'max'}).collect()[0][0]\n",
    "        \n",
    "        print(f'Número máximo de columnas \"stations\" encontradas \"{max_stations}\" para la columna \"{columna_stations}\".')\n",
    "        \n",
    "        for i in range(max_stations):\n",
    "            df_spark = df_spark.withColumn(f'{columna_stations}_{i+1}', col(columna_stations)[i])\n",
    "            \n",
    "        return df_spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la expansión de columnos \"station\" para la columna \"{columna_stations}\"')\n",
    "\n",
    "\n",
    "def mapear_ids_stations(df_spark, df_stations, columna_stations_base):\n",
    "    \n",
    "    try:\n",
    "        df_spark = df_spark.alias('df_fact')\n",
    "        df_stations = df_stations.alias('df_dim')\n",
    "        \n",
    "        columnas_eliminar = []\n",
    "        \n",
    "        for i in range(1,4):\n",
    "            columna_station = f'{columna_stations_base}_{i}'\n",
    "            columnas_eliminar.append(columna_station)\n",
    "            \n",
    "            df_spark = df_spark.join(\n",
    "                        df_stations, \n",
    "                        col(f'df_fact.{columna_station}') == col('df_dim.id'),\n",
    "                        'left'\n",
    "                        ).select(\n",
    "                            df_spark['*'],\n",
    "                            df_stations['id_stations'].alias(f'id_stations_{i}')\n",
    "                        )\n",
    "        \n",
    "        df_spark = df_spark.drop(*columnas_eliminar\n",
    "                                 )\n",
    "        print(f'Mapeo de IDs para columna \"stations\" realizado con éxito para la columna \"{columna_stations_base}\".')\n",
    "        return df_spark \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en el mapeo de IDs \"station\" para la columna \"{columna_stations_base}\": {e}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/'\n",
    "\n",
    "params = {\n",
    "    'locacion' : 'Sicilia',\n",
    "    'fecha' : datetime.now().strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "# EXTRAER Y GUARDAR DATOS CRUDOS\n",
    "file_path_key = 'api_key.txt'\n",
    "data = aplicar_extraccion_incremental(url, params, file_path_key)\n",
    "\n",
    "#data = extraer_datos_climaticos(url, params, file_path_key)\n",
    "guardar_archivos_datos(data)\n",
    "\n",
    "\n",
    "# PROCESAMIENTO DE DATOS\n",
    "data_dir = 'Datos'\n",
    "df = obtener_ultimo_archivo(data_dir, extension='.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfExplodedArray_Alerts_Days_1 = {}\n",
    "dfDesanidadoStruct_Days_2 = {}\n",
    "dfExplodeArray_DaysHours_DayStation_3 = {} \n",
    "dfDesanidadoStruct_DaysHours_4 = {} \n",
    "\n",
    "dfDesanidadoStruct_Current_Station_1 = {}\n",
    "dfDesanidadoStruct_Current_Station_2 = {}\n",
    "\n",
    "dfDesanidadoStruct_Stations_2 = {}\n",
    "\n",
    "# 1 - \n",
    "aplicar_dataframe('explotar', df, dfExplodedArray_Alerts_Days_1)\n",
    "aplicar_dataframe('desanidar', df, dfDesanidadoStruct_Current_Station_1)\n",
    "aplicar_dataframe('desanidar', dfExplodedArray_Alerts_Days_1, dfDesanidadoStruct_Days_2)\n",
    "aplicar_dataframe('explotar', dfDesanidadoStruct_Days_2, dfExplodeArray_DaysHours_DayStation_3)\n",
    "aplicar_dataframe('desanidar', dfExplodeArray_DaysHours_DayStation_3, dfDesanidadoStruct_DaysHours_4)\n",
    "aplicar_dataframe('desanidar', dfDesanidadoStruct_Current_Station_1, dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "aplicar_dataframe('explotar', dfDesanidadoStruct_Current_Station_1, dfDesanidadoStruct_Current_Station_2)\n",
    "\n",
    "# 2 -\n",
    "reemplazar_nulos(dfExplodedArray_Alerts_Days_1)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Days_2)\n",
    "reemplazar_nulos(dfExplodeArray_DaysHours_DayStation_3)\n",
    "reemplazar_nulos(dfDesanidadoStruct_DaysHours_4)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Current_Station_2)\n",
    "reemplazar_nulos(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# 3 - ELIMINACIÓN DE COLUMNAS STRUCT/ARRAY\n",
    "df_Days_2_eliminacionColumna = eliminar_columna(dfDesanidadoStruct_Days_2, 'days_hours', 'days')\n",
    "df_original = eliminar_columna(df, ['alerts', 'currentConditions', 'days', 'stations'])\n",
    "\n",
    "# 4 - ELIMINACIÓN DE CORCHETES DE COLUMNAS ARRAY\n",
    "df_Days_2_Final = eliminar_corchetes_array(df_Days_2_eliminacionColumna)\n",
    "df_Days_Hours_Final = eliminar_corchetes_array(dfDesanidadoStruct_DaysHours_4, 'days_hours')\n",
    "df_currentConditions_Final = eliminar_corchetes_array(dfDesanidadoStruct_Current_Station_1, 'currentConditions')\n",
    "\n",
    "# 5 - UNIFICACIONES\n",
    "dfUnificado_stations = unificar_df(dfDesanidadoStruct_Stations_2)\n",
    "\n",
    "# ASIGNACIÓN VALORES IDs INCREMENTALES A DF station\n",
    "df_asignacionID_Stations = asignar_ids_incrementales_stations(dfUnificado_stations, 'id', 'id_stations')\n",
    "\n",
    "# 6 - EXPANSIÓN Y MAPEO DE COLUMNAS STATIONS EN LOS DISTINTOS DFs\n",
    "df_currenConditions_expansion = expandir_stations(df_currentConditions_Final['currentConditions'], 'currentConditions_stations')\n",
    "df_Days_expansion = expandir_stations(df_Days_2_Final, 'days_stations')\n",
    "df_daysHours_expansion = expandir_stations(df_Days_Hours_Final['days_hours'], 'days_hours_stations')\n",
    "# 6.1 - MAPEO\n",
    "df_currentConditions_mapeo = mapear_ids_stations(df_currenConditions_expansion, df_asignacionID_Stations, 'currentConditions_stations')\n",
    "df_Days_mapeo = mapear_ids_stations(df_Days_expansion, df_asignacionID_Stations, 'days_stations')\n",
    "df_daysHours_mapeo = mapear_ids_stations(df_daysHours_expansion, df_asignacionID_Stations, 'days_hours_stations')\n",
    "\n",
    "# ELIMINACIÓN COLUMNAS YA UTILIZADAS PARA EL MAPEO DE IDs\n",
    "df_currentConditions_eliminacionColumnas = eliminar_columna(df_currentConditions_mapeo, ['currentConditions_stations'])\n",
    "df_Days_eliminacionColumna = eliminar_columna(df_Days_mapeo, ['days_stations'])\n",
    "df_daysHours_eliminacionColumna = eliminar_columna(df_daysHours_mapeo, ['days_hours_stations'])\n",
    "\n",
    "# REEMPLAZO DE NULOS LUEGO DE ASIGNACIONES DE IDs\n",
    "df_currentConditions_final = reemplazar_nulos(df_currentConditions_eliminacionColumnas)\n",
    "df_Days_final = reemplazar_nulos(df_Days_eliminacionColumna)\n",
    "df_daysHours_final = reemplazar_nulos(df_daysHours_eliminacionColumna)\n",
    "\n",
    "# TRANSFORMACIONES A PANDAS\n",
    "dfPandas_stations = transformar_dfPandas(df_asignacionID_Stations)\n",
    "dfPandas_currentConditions = transformar_dfPandas(df_currentConditions_final)\n",
    "dfPandas_Days = transformar_dfPandas(df_Days_final)\n",
    "dfPandas_DayHours = transformar_dfPandas(df_daysHours_final)\n",
    "dfPandas_Original = transformar_dfPandas(df_original)\n",
    "\n",
    "# # # GUARDAR DF DE PANDAS EN FORMATO CSV\n",
    "# guardar_csv(dfPandas_stations, 'Datos/Datos_Procesados/Stations','Stations')\n",
    "# guardar_csv(dfPandas_currentConditions, 'Datos/Datos_Procesados/CurrentConditions', 'CurrentConditions')\n",
    "# guardar_csv(dfPandas_Days, 'Datos/Datos_Procesados/Days', 'Days')\n",
    "# guardar_csv(dfPandas_DayHours, 'Datos/Datos_Procesados/DaysHours', 'DaysHours')\n",
    "# guardar_csv(dfPandas_Original, 'Datos/Datos_Procesados/Original', 'Original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_hours_cloudcover</th>\n",
       "      <th>days_hours_conditions</th>\n",
       "      <th>days_hours_datetime</th>\n",
       "      <th>days_hours_datetimeEpoch</th>\n",
       "      <th>days_hours_dew</th>\n",
       "      <th>days_hours_feelslike</th>\n",
       "      <th>days_hours_humidity</th>\n",
       "      <th>days_hours_icon</th>\n",
       "      <th>days_hours_precip</th>\n",
       "      <th>days_hours_precipprob</th>\n",
       "      <th>days_hours_preciptype</th>\n",
       "      <th>days_hours_pressure</th>\n",
       "      <th>days_hours_severerisk</th>\n",
       "      <th>days_hours_snow</th>\n",
       "      <th>days_hours_snowdepth</th>\n",
       "      <th>days_hours_solarenergy</th>\n",
       "      <th>days_hours_solarradiation</th>\n",
       "      <th>days_hours_source</th>\n",
       "      <th>days_hours_temp</th>\n",
       "      <th>days_hours_uvindex</th>\n",
       "      <th>days_hours_visibility</th>\n",
       "      <th>days_hours_winddir</th>\n",
       "      <th>days_hours_windgust</th>\n",
       "      <th>days_hours_windspeed</th>\n",
       "      <th>id_stations_1</th>\n",
       "      <th>id_stations_2</th>\n",
       "      <th>id_stations_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.5</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>1738796400</td>\n",
       "      <td>41.1</td>\n",
       "      <td>45.9</td>\n",
       "      <td>72.70</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sin Dato</td>\n",
       "      <td>1028.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>obs</td>\n",
       "      <td>49.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>72.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.4</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>1738800000</td>\n",
       "      <td>39.4</td>\n",
       "      <td>47.8</td>\n",
       "      <td>68.19</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sin Dato</td>\n",
       "      <td>1028.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>obs</td>\n",
       "      <td>49.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>153.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.6</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>1738803600</td>\n",
       "      <td>39.3</td>\n",
       "      <td>46.5</td>\n",
       "      <td>72.51</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sin Dato</td>\n",
       "      <td>1028.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>obs</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>166.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.3</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>1738807200</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>72.11</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sin Dato</td>\n",
       "      <td>1028.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>obs</td>\n",
       "      <td>47.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>156.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.2</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>1738810800</td>\n",
       "      <td>39.3</td>\n",
       "      <td>43.9</td>\n",
       "      <td>76.73</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sin Dato</td>\n",
       "      <td>1027.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>obs</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>147.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days_hours_cloudcover days_hours_conditions days_hours_datetime  \\\n",
       "0                   20.5      Partially cloudy            00:00:00   \n",
       "1                   54.4      Partially cloudy            01:00:00   \n",
       "2                   40.6      Partially cloudy            02:00:00   \n",
       "3                   73.3      Partially cloudy            03:00:00   \n",
       "4                   74.2      Partially cloudy            04:00:00   \n",
       "\n",
       "   days_hours_datetimeEpoch  days_hours_dew  days_hours_feelslike  \\\n",
       "0                1738796400            41.1                  45.9   \n",
       "1                1738800000            39.4                  47.8   \n",
       "2                1738803600            39.3                  46.5   \n",
       "3                1738807200            39.3                  45.1   \n",
       "4                1738810800            39.3                  43.9   \n",
       "\n",
       "   days_hours_humidity      days_hours_icon  days_hours_precip  \\\n",
       "0                72.70  partly-cloudy-night                0.0   \n",
       "1                68.19  partly-cloudy-night                0.0   \n",
       "2                72.51  partly-cloudy-night                0.0   \n",
       "3                72.11  partly-cloudy-night                0.0   \n",
       "4                76.73  partly-cloudy-night                0.0   \n",
       "\n",
       "   days_hours_precipprob days_hours_preciptype  days_hours_pressure  \\\n",
       "0                    0.0              Sin Dato               1028.1   \n",
       "1                    0.0              Sin Dato               1028.1   \n",
       "2                    0.0              Sin Dato               1028.1   \n",
       "3                    0.0              Sin Dato               1028.1   \n",
       "4                    0.0              Sin Dato               1027.1   \n",
       "\n",
       "   days_hours_severerisk  days_hours_snow  days_hours_snowdepth  \\\n",
       "0                   10.0              0.0                   0.0   \n",
       "1                   10.0              0.0                   0.0   \n",
       "2                   10.0              0.0                   0.0   \n",
       "3                   10.0              0.0                   0.0   \n",
       "4                   10.0              0.0                   0.0   \n",
       "\n",
       "   days_hours_solarenergy  days_hours_solarradiation days_hours_source  \\\n",
       "0                     0.0                        0.0               obs   \n",
       "1                     0.0                        0.0               obs   \n",
       "2                     0.0                        0.0               obs   \n",
       "3                     0.0                        0.0               obs   \n",
       "4                     0.0                        0.0               obs   \n",
       "\n",
       "   days_hours_temp  days_hours_uvindex  days_hours_visibility  \\\n",
       "0             49.5                 0.0                    6.2   \n",
       "1             49.5                 0.0                    6.2   \n",
       "2             47.7                 0.0                    6.2   \n",
       "3             47.8                 0.0                    6.2   \n",
       "4             46.2                 0.0                    6.2   \n",
       "\n",
       "   days_hours_winddir  days_hours_windgust  days_hours_windspeed  \\\n",
       "0                72.0                  6.9                   8.6   \n",
       "1               153.0                  9.2                   4.6   \n",
       "2               166.0                  9.6                   3.6   \n",
       "3               156.0                  9.8                   5.9   \n",
       "4               147.0                  9.8                   4.7   \n",
       "\n",
       "   id_stations_1  id_stations_2  id_stations_3  \n",
       "0              1              3              4  \n",
       "1              1              3              4  \n",
       "2              1              3              4  \n",
       "3              1              3              4  \n",
       "4              1              3              4  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPandas_DayHours.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Columna \"days_hours_datetime\" convertida a HORA (TIME)\n",
      "✔ Columna \"days_datetime\" convertida a FECHA (DATE)\n",
      "✔ Columna \"days_sunrise\" convertida a HORA (TIME)\n",
      "✔ Columna \"days_sunset\" convertida a HORA (TIME)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convertir_columnas_fecha_hora(df):\n",
    "    \"\"\"\n",
    "    Convierte las columnas de un DataFrame de Pandas que contienen fechas (YYYY-MM-DD) \n",
    "    o tiempos (HH:MM:SS) a tipos datetime64[ns], manteniéndolos diferenciados.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Evitar modificar el DataFrame original\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Solo procesar columnas de texto\n",
    "            \n",
    "            # Intentar convertir a formato de fecha (YYYY-MM-DD)\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='raise')\n",
    "                df[col] = df[col].dt.date  # Convertir a objeto date\n",
    "                df[col] = df[col].astype('datetime64[ns]')  # Forzar a datetime64 para mapearlo correctamente\n",
    "                print(f'✔ Columna \"{col}\" convertida a FECHA (DATE)')\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            # Intentar convertir a formato de hora (HH:MM:SS)\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%H:%M:%S', errors='raise')\n",
    "                print(f'✔ Columna \"{col}\" convertida a HORA (TIME)')\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "dfPandas_DayHours_2 = convertir_columnas_fecha_hora(dfPandas_DayHours)         \n",
    "dfPandas_Days_2 = convertir_columnas_fecha_hora(dfPandas_Days)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 27 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   days_hours_cloudcover      24 non-null     float64       \n",
      " 1   days_hours_conditions      24 non-null     object        \n",
      " 2   days_hours_datetime        24 non-null     datetime64[ns]\n",
      " 3   days_hours_datetimeEpoch   24 non-null     int64         \n",
      " 4   days_hours_dew             24 non-null     float64       \n",
      " 5   days_hours_feelslike       24 non-null     float64       \n",
      " 6   days_hours_humidity        24 non-null     float64       \n",
      " 7   days_hours_icon            24 non-null     object        \n",
      " 8   days_hours_precip          24 non-null     float64       \n",
      " 9   days_hours_precipprob      24 non-null     float64       \n",
      " 10  days_hours_preciptype      24 non-null     object        \n",
      " 11  days_hours_pressure        24 non-null     float64       \n",
      " 12  days_hours_severerisk      24 non-null     float64       \n",
      " 13  days_hours_snow            24 non-null     float64       \n",
      " 14  days_hours_snowdepth       24 non-null     float64       \n",
      " 15  days_hours_solarenergy     24 non-null     float64       \n",
      " 16  days_hours_solarradiation  24 non-null     float64       \n",
      " 17  days_hours_source          24 non-null     object        \n",
      " 18  days_hours_temp            24 non-null     float64       \n",
      " 19  days_hours_uvindex         24 non-null     float64       \n",
      " 20  days_hours_visibility      24 non-null     float64       \n",
      " 21  days_hours_winddir         24 non-null     float64       \n",
      " 22  days_hours_windgust        24 non-null     float64       \n",
      " 23  days_hours_windspeed       24 non-null     float64       \n",
      " 24  id_stations_1              24 non-null     int32         \n",
      " 25  id_stations_2              24 non-null     int32         \n",
      " 26  id_stations_3              24 non-null     int32         \n",
      "dtypes: datetime64[ns](1), float64(18), int32(3), int64(1), object(4)\n",
      "memory usage: 4.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dfPandas_DayHours_2.hae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#dfPandas_DayHours['days_hours_datetime'] = pd.to_datetime(dfPandas_DayHours['days_hours_datetime'], format = '%H:%M:%S')\n",
    "# dfPandas_DayHours['days_hours_datetime'] = dfPandas_DayHours['days_hours_datetime'].dt.time\n",
    "# dfPandas_DayHours['days_hours_datetime'] = dfPandas_DayHours['days_hours_datetime'].astype('datetime64[ns]')\n",
    "print(dfPandas_DayHours_2['days_hours_datetime'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 27 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   days_hours_cloudcover      24 non-null     float64       \n",
      " 1   days_hours_conditions      24 non-null     object        \n",
      " 2   days_hours_datetime        24 non-null     datetime64[ns]\n",
      " 3   days_hours_datetimeEpoch   24 non-null     int64         \n",
      " 4   days_hours_dew             24 non-null     float64       \n",
      " 5   days_hours_feelslike       24 non-null     float64       \n",
      " 6   days_hours_humidity        24 non-null     float64       \n",
      " 7   days_hours_icon            24 non-null     object        \n",
      " 8   days_hours_precip          24 non-null     float64       \n",
      " 9   days_hours_precipprob      24 non-null     float64       \n",
      " 10  days_hours_preciptype      24 non-null     object        \n",
      " 11  days_hours_pressure        24 non-null     float64       \n",
      " 12  days_hours_severerisk      24 non-null     float64       \n",
      " 13  days_hours_snow            24 non-null     float64       \n",
      " 14  days_hours_snowdepth       24 non-null     float64       \n",
      " 15  days_hours_solarenergy     24 non-null     float64       \n",
      " 16  days_hours_solarradiation  24 non-null     float64       \n",
      " 17  days_hours_source          24 non-null     object        \n",
      " 18  days_hours_temp            24 non-null     float64       \n",
      " 19  days_hours_uvindex         24 non-null     float64       \n",
      " 20  days_hours_visibility      24 non-null     float64       \n",
      " 21  days_hours_winddir         24 non-null     float64       \n",
      " 22  days_hours_windgust        24 non-null     float64       \n",
      " 23  days_hours_windspeed       24 non-null     float64       \n",
      " 24  id_stations_1              24 non-null     int32         \n",
      " 25  id_stations_2              24 non-null     int32         \n",
      " 26  id_stations_3              24 non-null     int32         \n",
      "dtypes: datetime64[ns](1), float64(18), int32(3), int64(1), object(4)\n",
      "memory usage: 4.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dfPandas_DayHours_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE ARCHIVOS A MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando conexión a MySQL.\n",
      "Conexión exitosa\n",
      "Datos insertados correctamente en la tabla stations.\n",
      "Datos insertados correctamente en la tabla current_conditions.\n",
      "Datos insertados correctamente en la tabla days.\n",
      "Datos insertados correctamente en la tabla days_hours.\n",
      "Datos insertados correctamente en la tabla city.\n",
      "Relaciones en \"stations_relations\" insertadas correctamente.\n",
      "Datos insertados exitosamente\n",
      "Conexión cerrada con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "user = os.environ.get('MYSQL_USER')\n",
    "password = os.environ.get('MYSQL_PASSWORD')\n",
    "host = os.environ.get('MYSQL_HOST')\n",
    "database = os.environ.get('MYSQL_DATABASE')\n",
    "port = int(os.environ.get('MYSQL_PORT', 3306))\n",
    "\n",
    "\n",
    "def insertar_datos_a_tabla(cursor, tabla, columnas, datos):\n",
    "    \n",
    "    try:\n",
    "        placeholders = ', '.join(['%s'] * len(columnas))\n",
    "        insert_query = f\"INSERT INTO {tabla} ({', '.join(columnas)}) VALUES ({placeholders})\"\n",
    "\n",
    "        cursor.executemany(insert_query, datos)\n",
    "        print(f'Datos insertados correctamente en la tabla {tabla}.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error en la inserción de datos: \"{tabla}\" / {e}')\n",
    "\n",
    "\n",
    "def crear_tablas(cursor):\n",
    "    \n",
    "    try:\n",
    "        tablas = [\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS stations (\n",
    "               contribution FLOAT,\n",
    "               distance FLOAT,\n",
    "               id VARCHAR(15) UNIQUE,\n",
    "               latitude FLOAT,\n",
    "               longitude FLOAT,\n",
    "               name VARCHAR(50),\n",
    "               quality INT,\n",
    "               useCount INT,\n",
    "               id_stations INT\n",
    "            );\n",
    "            ''',\n",
    "            '''\n",
    "            CREATE TABLE IF NOT EXISTS current_conditions (\n",
    "            id_currentConditions INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            currentConditions_cloudcover FLOAT,\n",
    "            currentConditions_conditions VARCHAR(250),\n",
    "            currentConditions_datetime TIME,\n",
    "            currentConditions_datetimeEpoch INT, \n",
    "            currentConditions_dew FLOAT,\n",
    "            currentConditions_feelslike FLOAT,\n",
    "            currentConditions_humidity FLOAT,\n",
    "            currentConditions_icon VARCHAR(100),\n",
    "            currentConditions_moonphase FLOAT,\n",
    "            currentConditions_precip FLOAT,\n",
    "            currentConditions_precipprob FLOAT,\n",
    "            currentConditions_preciptype VARCHAR(100),\n",
    "            currentConditions_pressure FLOAT,\n",
    "            currentConditions_snow FLOAT, \n",
    "            currentConditions_snowdepth FLOAT,\n",
    "            currentConditions_solarenergy FLOAT,\n",
    "            currentConditions_solarradiation FLOAT,\n",
    "            currentConditions_source VARCHAR(50),\n",
    "            currentConditions_sunrise TIME,\n",
    "            currentConditions_sunriseEpoch INT, \n",
    "            currentConditions_sunset TIME,\n",
    "            currentConditions_sunsetEpoch INT,\n",
    "            currentConditions_temp FLOAT,\n",
    "            currentConditions_uvindex FLOAT, \n",
    "            currentConditions_visibility FLOAT,\n",
    "            currentConditions_winddir FLOAT,\n",
    "            currentConditions_windgust FLOAT, \n",
    "            currentConditions_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days (\n",
    "                id_days INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_cloudcover FLOAT,\n",
    "                days_conditions VARCHAR(200),\n",
    "                days_datetime DATE,\n",
    "                days_datetimeEpoch INT,\n",
    "                days_description VARCHAR(200),\n",
    "                days_dew FLOAT,\n",
    "                days_feelslike FLOAT,\n",
    "                days_feelslikemax FLOAT, \n",
    "                days_feelslikemin FLOAT, \n",
    "                days_humidity FLOAT, \n",
    "                days_icon VARCHAR(50),\n",
    "                days_moonphase FLOAT, \n",
    "                days_precip FLOAT, \n",
    "                days_precipcover FLOAT, \n",
    "                days_precipprob FLOAT,\n",
    "                days_preciptype VARCHAR(50),\n",
    "                days_pressure FLOAT, \n",
    "                days_severerisk FLOAT, \n",
    "                days_snow FLOAT, \n",
    "                days_snowdepth FLOAT, \n",
    "                days_solarenergy FLOAT,\n",
    "                days_solarradiation FLOAT,\n",
    "                days_source VARCHAR(50),\n",
    "                days_stations VARCHAR(200),\n",
    "                days_sunrise TIME,\n",
    "                days_sunriseEpoch INT,\n",
    "                days_sunset TIME,\n",
    "                days_sunsetEpoch INT,\n",
    "                days_temp FLOAT, \n",
    "                days_tempmax FLOAT,\n",
    "                days_tempmin FLOAT,\n",
    "                days_uvindex FLOAT,\n",
    "                days_visibility FLOAT,\n",
    "                days_winddir FLOAT,\n",
    "                days_windgust FLOAT,\n",
    "                days_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS days_hours (\n",
    "                id_daysHours INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                days_hours_cloudcover FLOAT,\n",
    "                days_hours_conditions VARCHAR(200),\n",
    "                days_hours_datetime TIME,\n",
    "                days_hours_datetimeEpoch INT, \n",
    "                days_hours_dew FLOAT,\n",
    "                days_hours_feelslike FLOAT,\n",
    "                days_hours_humidity FLOAT,\n",
    "                days_hours_icon VARCHAR(200),\n",
    "                days_hours_precip FLOAT,\n",
    "                days_hours_precipprob FLOAT,\n",
    "                days_hours_preciptype VARCHAR(200),\n",
    "                days_hours_pressure FLOAT,\n",
    "                days_hours_severerisk FLOAT,\n",
    "                days_hours_snow FLOAT,\n",
    "                days_hours_snowdepth FLOAT,\n",
    "                days_hours_solarenergy FLOAT,\n",
    "                days_hours_solarradiation FLOAT,\n",
    "                days_hours_source VARCHAR(50),\n",
    "                days_hours_stations VARCHAR(200),\n",
    "                days_hours_temp FLOAT,\n",
    "                days_hours_uvindex FLOAT,\n",
    "                days_hours_visibility FLOAT,\n",
    "                days_hours_winddir FLOAT,\n",
    "                days_hours_windgust FLOAT,\n",
    "                days_hours_windspeed FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS city (\n",
    "                id_city INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                address VARCHAR(50),\n",
    "                description VARCHAR(300),\n",
    "                latitude FLOAT,\n",
    "                longitude FLOAT,\n",
    "                queryCost INT, \n",
    "                resolvedAddress VARCHAR(100),\n",
    "                timezone VARCHAR(100),\n",
    "                tzoffset FLOAT\n",
    "            );\n",
    "            ''',\n",
    "            ''' \n",
    "            CREATE TABLE IF NOT EXISTS stations_relations (\n",
    "            id_relations INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            id_stations INT NULL,\n",
    "            id_currentConditions INT NULL,\n",
    "            id_days INT NULL,\n",
    "            id_daysHours INT NULL,\n",
    "            FOREIGN KEY (id_stations) REFERENCES stations(id_stations),\n",
    "            FOREIGN KEY (id_currentConditions) REFERENCES current_conditions(id_currentConditions),\n",
    "            FOREIGN KEY (id_days) REFERENCES days(id_days),\n",
    "            FOREIGN KEY (id_daysHours) REFERENCES days_hours(id_daysHours)\n",
    "            );\n",
    "            '''\n",
    "        ]\n",
    "        \n",
    "        for query in tablas:\n",
    "            cursor.execute(query)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la creación de tablas: {e}')\n",
    "\n",
    "\n",
    "def insertar_relaciones(cursor):\n",
    "    \n",
    "    try:\n",
    "        query = ''' \n",
    "            INSERT INTO stations_relations (id_stations, id_currentConditions, id_days, id_daysHours)\n",
    "            SELECT DISTINCT s.id_stations, c.id_currentConditions, d.id_days, dh.id_daysHours\n",
    "            FROM stations s\n",
    "            LEFT JOIN current_conditions c ON FIND_IN_SET(s.id, c.currentConditions_stations)\n",
    "            LEFT JOIN days d ON FIND_IN_SET(s.id, d.days_stations)\n",
    "            LEFT JOIN days_hours dh ON FIND_IN_SET(s.id, dh.days_hours_stations);        \n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        print('Relaciones en \"stations_relations\" insertadas correctamente.')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error insertando relaciones: {e}')\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    print('Intentando conexión a MySQL.')\n",
    "    \n",
    "    try:\n",
    "        conexion = pymysql.connect(user= user, password= password,\n",
    "                                        host= host,   \n",
    "                                        database = database,\n",
    "                                        port = port)\n",
    "        print(\"Conexión exitosa\")\n",
    "    \n",
    "        cursor = conexion.cursor()\n",
    "        \n",
    "        crear_tablas(cursor)\n",
    "        \n",
    "        dataframes = {\n",
    "            'stations' : dfPandas_stations,\n",
    "            'current_conditions' : dfPandas_currentConditions,\n",
    "            'days' : dfPandas_Days,\n",
    "            'days_hours' : dfPandas_DayHours,\n",
    "            'city' : dfPandas_Original\n",
    "        }\n",
    "        \n",
    "        for tabla, df in dataframes.items():\n",
    "            \n",
    "            columnas = df.columns.tolist()\n",
    "            datos = [tuple(row) for row in df.values]\n",
    "        \n",
    "            insertar_datos_a_tabla(cursor, tabla, columnas, datos)\n",
    "        \n",
    "        insertar_relaciones(cursor)\n",
    "        conexion.commit()\n",
    "        print('Datos insertados exitosamente')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión: {e}')\n",
    "        \n",
    "    finally: \n",
    "        conexion.close()\n",
    "        print('Conexión cerrada con éxito.')\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA DE DATOS A MYSQL \"2\"\n",
    "\n",
    "def mapear_tipos_datos_mysql(df):\n",
    "    \n",
    "    tipos_mysql = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        \n",
    "        if dtype.startswith('datetime64'):\n",
    "            \n",
    "            if df[col].dt.strftime('%H:%M:%S').nunique() == 1 and '00:00:00' in df[col].dt.strftime('%H:%M:%S').values:\n",
    "                tipos_mysql[col] = 'DATE'\n",
    "            \n",
    "            elif df[col].dt.strftime('%Y-%m-%d').nunique() == 1 and '1900-01-01' in df[col].dt.strftime('%Y-%m-%d').values:\n",
    "                tipos_mysql[col] = 'TIME'\n",
    "            \n",
    "            else: \n",
    "                tipos_mysql[col] = 'DATETIME'\n",
    "\n",
    "        elif dtype == 'object':\n",
    "            tipos_mysql[col] = 'VARCHAR(400)'\n",
    "        \n",
    "        elif dtype.startswith('int'):\n",
    "            tipos_mysql[col] = 'INT'\n",
    "        \n",
    "        elif dtype.startswith('float'):\n",
    "            tipos_mysql[col] = 'FLOAT'\n",
    "        \n",
    "        else:\n",
    "            tipos_mysql[col] = 'VARCHAR(400)'\n",
    "        \n",
    "    return tipos_mysql\n",
    "\n",
    "\n",
    "\n",
    "tipos_hours = mapear_tipos_datos_mysql(dfPandas_DayHours_2)\n",
    "tipos_days = mapear_tipos_datos_mysql(dfPandas_Days_2)\n",
    "\n",
    "# 1 - REALIZAR LAS NUEVAS FUNCIONES LAS CUALES CREEN TABLAS DINAMICAMENTE EN MYSQL\n",
    "# 1.2 - ¿CÓMO APLICAR LAS RELACIONES SI YA HAY FK EN 3 DFs?\n",
    "# 1.3 - ¿CÓMO APLICAR IDs AUTOINCREMENTALES A 4 TABLAS?\n",
    "# 2 - COMPRENDER SI SE REALIZA DE MANERA CORRECTA LA INSERCION DE DATOS A TABLA \"stations_relations\"\n",
    "# 2.2 - ENTENDER COMO FUNCIONA \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
